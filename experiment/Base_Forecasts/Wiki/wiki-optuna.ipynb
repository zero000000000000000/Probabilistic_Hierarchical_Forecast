{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":8195619,"sourceType":"datasetVersion","datasetId":4854236}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install 'gluonts[torch]'","metadata":{"execution":{"iopub.status.busy":"2024-05-06T13:39:05.614766Z","iopub.execute_input":"2024-05-06T13:39:05.615118Z","iopub.status.idle":"2024-05-06T13:39:25.103246Z","shell.execute_reply.started":"2024-05-06T13:39:05.615089Z","shell.execute_reply":"2024-05-06T13:39:25.102326Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting gluonts[torch]\n  Downloading gluonts-0.14.4-py3-none-any.whl.metadata (9.5 kB)\nRequirement already satisfied: numpy~=1.16 in /opt/conda/lib/python3.10/site-packages (from gluonts[torch]) (1.26.4)\nRequirement already satisfied: pandas<2.2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from gluonts[torch]) (2.1.4)\nRequirement already satisfied: pydantic<3,>=1.7 in /opt/conda/lib/python3.10/site-packages (from gluonts[torch]) (2.5.3)\nRequirement already satisfied: tqdm~=4.23 in /opt/conda/lib/python3.10/site-packages (from gluonts[torch]) (4.66.1)\nRequirement already satisfied: toolz~=0.10 in /opt/conda/lib/python3.10/site-packages (from gluonts[torch]) (0.12.1)\nRequirement already satisfied: typing-extensions~=4.0 in /opt/conda/lib/python3.10/site-packages (from gluonts[torch]) (4.9.0)\nRequirement already satisfied: torch<3,>=1.9 in /opt/conda/lib/python3.10/site-packages (from gluonts[torch]) (2.1.2)\nCollecting lightning<2.2,>=2.0 (from gluonts[torch])\n  Downloading lightning-2.1.4-py3-none-any.whl.metadata (57 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.2/57.2 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting pytorch-lightning<2.2,>=2.0 (from gluonts[torch])\n  Downloading pytorch_lightning-2.1.4-py3-none-any.whl.metadata (21 kB)\nRequirement already satisfied: scipy~=1.10 in /opt/conda/lib/python3.10/site-packages (from gluonts[torch]) (1.11.4)\nRequirement already satisfied: PyYAML<8.0,>=5.4 in /opt/conda/lib/python3.10/site-packages (from lightning<2.2,>=2.0->gluonts[torch]) (6.0.1)\nRequirement already satisfied: fsspec<2025.0,>=2022.5.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<2025.0,>=2022.5.0->lightning<2.2,>=2.0->gluonts[torch]) (2024.2.0)\nRequirement already satisfied: lightning-utilities<2.0,>=0.8.0 in /opt/conda/lib/python3.10/site-packages (from lightning<2.2,>=2.0->gluonts[torch]) (0.11.2)\nRequirement already satisfied: packaging<25.0,>=20.0 in /opt/conda/lib/python3.10/site-packages (from lightning<2.2,>=2.0->gluonts[torch]) (21.3)\nRequirement already satisfied: torchmetrics<3.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from lightning<2.2,>=2.0->gluonts[torch]) (1.3.2)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas<2.2.0,>=1.0->gluonts[torch]) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas<2.2.0,>=1.0->gluonts[torch]) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas<2.2.0,>=1.0->gluonts[torch]) (2023.4)\nRequirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from pydantic<3,>=1.7->gluonts[torch]) (0.6.0)\nRequirement already satisfied: pydantic-core==2.14.6 in /opt/conda/lib/python3.10/site-packages (from pydantic<3,>=1.7->gluonts[torch]) (2.14.6)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch<3,>=1.9->gluonts[torch]) (3.13.1)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch<3,>=1.9->gluonts[torch]) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch<3,>=1.9->gluonts[torch]) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch<3,>=1.9->gluonts[torch]) (3.1.2)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<2025.0,>=2022.5.0->lightning<2.2,>=2.0->gluonts[torch]) (3.9.1)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from lightning-utilities<2.0,>=0.8.0->lightning<2.2,>=2.0->gluonts[torch]) (69.0.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging<25.0,>=20.0->lightning<2.2,>=2.0->gluonts[torch]) (3.1.1)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas<2.2.0,>=1.0->gluonts[torch]) (1.16.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch<3,>=1.9->gluonts[torch]) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch<3,>=1.9->gluonts[torch]) (1.3.0)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning<2.2,>=2.0->gluonts[torch]) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning<2.2,>=2.0->gluonts[torch]) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning<2.2,>=2.0->gluonts[torch]) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning<2.2,>=2.0->gluonts[torch]) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning<2.2,>=2.0->gluonts[torch]) (1.3.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning<2.2,>=2.0->gluonts[torch]) (4.0.3)\nRequirement already satisfied: idna>=2.0 in /opt/conda/lib/python3.10/site-packages (from yarl<2.0,>=1.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning<2.2,>=2.0->gluonts[torch]) (3.6)\nDownloading lightning-2.1.4-py3-none-any.whl (2.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n\u001b[?25hDownloading pytorch_lightning-2.1.4-py3-none-any.whl (778 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m778.1/778.1 kB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading gluonts-0.14.4-py3-none-any.whl (1.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m28.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: gluonts, pytorch-lightning, lightning\n  Attempting uninstall: pytorch-lightning\n    Found existing installation: pytorch-lightning 2.2.2\n    Uninstalling pytorch-lightning-2.2.2:\n      Successfully uninstalled pytorch-lightning-2.2.2\nSuccessfully installed gluonts-0.14.4 lightning-2.1.4 pytorch-lightning-2.1.4\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install optuna","metadata":{"execution":{"iopub.status.busy":"2024-05-06T13:39:40.649953Z","iopub.execute_input":"2024-05-06T13:39:40.650803Z","iopub.status.idle":"2024-05-06T13:39:53.654330Z","shell.execute_reply.started":"2024-05-06T13:39:40.650766Z","shell.execute_reply":"2024-05-06T13:39:53.653177Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Requirement already satisfied: optuna in /opt/conda/lib/python3.10/site-packages (3.6.1)\nRequirement already satisfied: alembic>=1.5.0 in /opt/conda/lib/python3.10/site-packages (from optuna) (1.13.1)\nRequirement already satisfied: colorlog in /opt/conda/lib/python3.10/site-packages (from optuna) (6.8.2)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from optuna) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from optuna) (21.3)\nRequirement already satisfied: sqlalchemy>=1.3.0 in /opt/conda/lib/python3.10/site-packages (from optuna) (2.0.25)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from optuna) (4.66.1)\nRequirement already satisfied: PyYAML in /opt/conda/lib/python3.10/site-packages (from optuna) (6.0.1)\nRequirement already satisfied: Mako in /opt/conda/lib/python3.10/site-packages (from alembic>=1.5.0->optuna) (1.3.3)\nRequirement already satisfied: typing-extensions>=4 in /opt/conda/lib/python3.10/site-packages (from alembic>=1.5.0->optuna) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->optuna) (3.1.1)\nRequirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.10/site-packages (from sqlalchemy>=1.3.0->optuna) (3.0.3)\nRequirement already satisfied: MarkupSafe>=0.9.2 in /opt/conda/lib/python3.10/site-packages (from Mako->alembic>=1.5.0->optuna) (2.1.3)\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom gluonts.dataset.pandas import PandasDataset\nfrom gluonts.torch.model.deepar import DeepAREstimator\n#from gluonts.torch.distributions import NegativeBinomialOutput\n#from gluonts.torch.distributions import PoissonOutput\nfrom gluonts.torch.distributions import NormalOutput\nfrom gluonts.evaluation import make_evaluation_predictions\nimport matplotlib.pyplot as plt\nimport json\n\n# Split train and test\ndf = pd.read_csv('../input/wiki-data/Wiki_process_for_deepar.csv')\ndf.set_index('Date',inplace=True)\n#df.index = pd.to_datetime(df.index).dt.strftime('%Y-%m-%d %H:%M:%S')\nprediction_length = 15\nfreq = 'D'\nsplit_date = pd.to_datetime('2016-12-17').strftime('%Y-%m-%d %H:%M:%S')\ntrain = df[df.index < split_date]\ntest = df[df.index >= split_date]","metadata":{"execution":{"iopub.status.busy":"2024-05-06T13:39:56.248558Z","iopub.execute_input":"2024-05-06T13:39:56.249227Z","iopub.status.idle":"2024-05-06T13:40:04.618751Z","shell.execute_reply.started":"2024-05-06T13:39:56.249193Z","shell.execute_reply":"2024-05-06T13:40:04.617758Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"train.shape,test.shape","metadata":{"execution":{"iopub.status.busy":"2024-05-06T13:40:59.974039Z","iopub.execute_input":"2024-05-06T13:40:59.974421Z","iopub.status.idle":"2024-05-06T13:40:59.980787Z","shell.execute_reply.started":"2024-05-06T13:40:59.974390Z","shell.execute_reply":"2024-05-06T13:40:59.979762Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"((69849, 9), (2985, 9))"},"metadata":{}}]},{"cell_type":"code","source":"split_date1 = pd.to_datetime('2016-12-02').strftime('%Y-%m-%d %H:%M:%S')\ntrain1 = df[df.index < split_date1]\nval = train[train.index>=split_date1]","metadata":{"execution":{"iopub.status.busy":"2024-05-06T14:21:52.099558Z","iopub.execute_input":"2024-05-06T14:21:52.100437Z","iopub.status.idle":"2024-05-06T14:21:52.130251Z","shell.execute_reply.started":"2024-05-06T14:21:52.100403Z","shell.execute_reply":"2024-05-06T14:21:52.129503Z"},"trusted":true},"execution_count":67,"outputs":[]},{"cell_type":"code","source":"train1.shape,val.shape","metadata":{"execution":{"iopub.status.busy":"2024-05-06T14:21:54.374199Z","iopub.execute_input":"2024-05-06T14:21:54.374530Z","iopub.status.idle":"2024-05-06T14:21:54.380393Z","shell.execute_reply.started":"2024-05-06T14:21:54.374505Z","shell.execute_reply":"2024-05-06T14:21:54.379519Z"},"trusted":true},"execution_count":68,"outputs":[{"execution_count":68,"output_type":"execute_result","data":{"text/plain":"((66864, 9), (2985, 9))"},"metadata":{}}]},{"cell_type":"code","source":"train1.head()","metadata":{"execution":{"iopub.status.busy":"2024-05-06T14:21:58.319554Z","iopub.execute_input":"2024-05-06T14:21:58.319922Z","iopub.status.idle":"2024-05-06T14:21:58.332819Z","shell.execute_reply.started":"2024-05-06T14:21:58.319891Z","shell.execute_reply":"2024-05-06T14:21:58.331646Z"},"trusted":true},"execution_count":69,"outputs":[{"execution_count":69,"output_type":"execute_result","data":{"text/plain":"                      Node   Value  Day_of_Week  Day_Of_Month  Month_Of_Year  \\\nDate                                                                           \n2016-01-01 00:00:00  Total  156508            4             1              1   \n2016-01-01 00:00:00     de   15342            4             1              1   \n2016-01-01 00:00:00     en   63319            4             1              1   \n2016-01-01 00:00:00     fr   33489            4             1              1   \n2016-01-01 00:00:00     ja   17242            4             1              1   \n\n                     Code  Agent  Access  State  \nDate                                             \n2016-01-01 00:00:00     0      0       0      0  \n2016-01-01 00:00:00     0      0       0      1  \n2016-01-01 00:00:00     0      0       0      2  \n2016-01-01 00:00:00     0      0       0      3  \n2016-01-01 00:00:00     0      0       0      4  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Node</th>\n      <th>Value</th>\n      <th>Day_of_Week</th>\n      <th>Day_Of_Month</th>\n      <th>Month_Of_Year</th>\n      <th>Code</th>\n      <th>Agent</th>\n      <th>Access</th>\n      <th>State</th>\n    </tr>\n    <tr>\n      <th>Date</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2016-01-01 00:00:00</th>\n      <td>Total</td>\n      <td>156508</td>\n      <td>4</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2016-01-01 00:00:00</th>\n      <td>de</td>\n      <td>15342</td>\n      <td>4</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2016-01-01 00:00:00</th>\n      <td>en</td>\n      <td>63319</td>\n      <td>4</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>2016-01-01 00:00:00</th>\n      <td>fr</td>\n      <td>33489</td>\n      <td>4</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>2016-01-01 00:00:00</th>\n      <td>ja</td>\n      <td>17242</td>\n      <td>4</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>4</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"train1.reset_index(inplace=True)\nval.reset_index(inplace=True)\n\ntrain2 = train1.drop_duplicates(subset=['Node','Code','Agent','Access','State'])\ntrain_static = pd.DataFrame({'State':train2['State'],\n                             'Access':train2['Access'],\n                             'Agent':train2['Agent'],\n                             'Code':train2['Code'],\n                             'Node':train2['Node']})\ntrain_static.set_index('Node',inplace=True)\n\ntrain_group = train1.groupby('Node')\nstandardized_params = {}\ntrain_standard = train1.copy()\n\n","metadata":{"execution":{"iopub.status.busy":"2024-05-06T14:22:03.064460Z","iopub.execute_input":"2024-05-06T14:22:03.065237Z","iopub.status.idle":"2024-05-06T14:22:03.095500Z","shell.execute_reply.started":"2024-05-06T14:22:03.065204Z","shell.execute_reply":"2024-05-06T14:22:03.094301Z"},"trusted":true},"execution_count":70,"outputs":[]},{"cell_type":"code","source":"train_standard['Value'] = train_standard['Value'].astype(float)","metadata":{"execution":{"iopub.status.busy":"2024-05-06T14:22:18.784912Z","iopub.execute_input":"2024-05-06T14:22:18.785360Z","iopub.status.idle":"2024-05-06T14:22:18.794836Z","shell.execute_reply.started":"2024-05-06T14:22:18.785326Z","shell.execute_reply":"2024-05-06T14:22:18.793928Z"},"trusted":true},"execution_count":72,"outputs":[]},{"cell_type":"code","source":"train_standard","metadata":{"execution":{"iopub.status.busy":"2024-05-06T14:22:20.472853Z","iopub.execute_input":"2024-05-06T14:22:20.473942Z","iopub.status.idle":"2024-05-06T14:22:20.489754Z","shell.execute_reply.started":"2024-05-06T14:22:20.473898Z","shell.execute_reply":"2024-05-06T14:22:20.488771Z"},"trusted":true},"execution_count":73,"outputs":[{"execution_count":73,"output_type":"execute_result","data":{"text/plain":"                      Date            Node     Value  Day_of_Week  \\\n0      2016-01-01 00:00:00           Total  156508.0            4   \n1      2016-01-01 00:00:00              de   15342.0            4   \n2      2016-01-01 00:00:00              en   63319.0            4   \n3      2016-01-01 00:00:00              fr   33489.0            4   \n4      2016-01-01 00:00:00              ja   17242.0            4   \n...                    ...             ...       ...          ...   \n66859  2016-12-01 00:00:00  zh_MOB_AAG_005      15.0            3   \n66860  2016-12-01 00:00:00  zh_MOB_AAG_028       9.0            3   \n66861  2016-12-01 00:00:00  zh_MOB_AAG_031     151.0            3   \n66862  2016-12-01 00:00:00  zh_MOB_AAG_036      67.0            3   \n66863  2016-12-01 00:00:00  zh_MOB_AAG_138      66.0            3   \n\n       Day_Of_Month  Month_Of_Year  Code  Agent  Access  State  \n0                 1              1     0      0       0      0  \n1                 1              1     0      0       0      1  \n2                 1              1     0      0       0      2  \n3                 1              1     0      0       0      3  \n4                 1              1     0      0       0      4  \n...             ...            ...   ...    ...     ...    ...  \n66859             1             12   146     24      18      6  \n66860             1             12   147     24      18      6  \n66861             1             12   148     24      18      6  \n66862             1             12   149     24      18      6  \n66863             1             12   150     24      18      6  \n\n[66864 rows x 10 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Date</th>\n      <th>Node</th>\n      <th>Value</th>\n      <th>Day_of_Week</th>\n      <th>Day_Of_Month</th>\n      <th>Month_Of_Year</th>\n      <th>Code</th>\n      <th>Agent</th>\n      <th>Access</th>\n      <th>State</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2016-01-01 00:00:00</td>\n      <td>Total</td>\n      <td>156508.0</td>\n      <td>4</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2016-01-01 00:00:00</td>\n      <td>de</td>\n      <td>15342.0</td>\n      <td>4</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2016-01-01 00:00:00</td>\n      <td>en</td>\n      <td>63319.0</td>\n      <td>4</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2016-01-01 00:00:00</td>\n      <td>fr</td>\n      <td>33489.0</td>\n      <td>4</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2016-01-01 00:00:00</td>\n      <td>ja</td>\n      <td>17242.0</td>\n      <td>4</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>66859</th>\n      <td>2016-12-01 00:00:00</td>\n      <td>zh_MOB_AAG_005</td>\n      <td>15.0</td>\n      <td>3</td>\n      <td>1</td>\n      <td>12</td>\n      <td>146</td>\n      <td>24</td>\n      <td>18</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>66860</th>\n      <td>2016-12-01 00:00:00</td>\n      <td>zh_MOB_AAG_028</td>\n      <td>9.0</td>\n      <td>3</td>\n      <td>1</td>\n      <td>12</td>\n      <td>147</td>\n      <td>24</td>\n      <td>18</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>66861</th>\n      <td>2016-12-01 00:00:00</td>\n      <td>zh_MOB_AAG_031</td>\n      <td>151.0</td>\n      <td>3</td>\n      <td>1</td>\n      <td>12</td>\n      <td>148</td>\n      <td>24</td>\n      <td>18</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>66862</th>\n      <td>2016-12-01 00:00:00</td>\n      <td>zh_MOB_AAG_036</td>\n      <td>67.0</td>\n      <td>3</td>\n      <td>1</td>\n      <td>12</td>\n      <td>149</td>\n      <td>24</td>\n      <td>18</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>66863</th>\n      <td>2016-12-01 00:00:00</td>\n      <td>zh_MOB_AAG_138</td>\n      <td>66.0</td>\n      <td>3</td>\n      <td>1</td>\n      <td>12</td>\n      <td>150</td>\n      <td>24</td>\n      <td>18</td>\n      <td>6</td>\n    </tr>\n  </tbody>\n</table>\n<p>66864 rows × 10 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# 标准化\nfor cat,group in train_group:\n    means = group['Value'].mean()\n    stds = group['Value'].std()\n    standardized_params[cat] = {'mean':means,'std':stds}\n    train_standard.loc[group.index,'Value'] = (group['Value']-means)/stds","metadata":{"execution":{"iopub.status.busy":"2024-05-06T14:22:24.661323Z","iopub.execute_input":"2024-05-06T14:22:24.662271Z","iopub.status.idle":"2024-05-06T14:22:24.911214Z","shell.execute_reply.started":"2024-05-06T14:22:24.662227Z","shell.execute_reply":"2024-05-06T14:22:24.910420Z"},"trusted":true},"execution_count":74,"outputs":[]},{"cell_type":"code","source":"train_ds = PandasDataset.from_long_dataframe(train_standard.iloc[:,[0,1,2,3,4,5]],\n                                             target=\"Value\",\n                                             timestamp='Date',\n                                             freq='D',\n                                             item_id=\"Node\",\n                                             feat_dynamic_real=['Day_of_Week','Day_Of_Month','Month_Of_Year'],\n                                             static_features=train_static)","metadata":{"execution":{"iopub.status.busy":"2024-05-06T14:22:27.408949Z","iopub.execute_input":"2024-05-06T14:22:27.409325Z","iopub.status.idle":"2024-05-06T14:22:27.462411Z","shell.execute_reply.started":"2024-05-06T14:22:27.409296Z","shell.execute_reply":"2024-05-06T14:22:27.461596Z"},"trusted":true},"execution_count":75,"outputs":[]},{"cell_type":"code","source":"val_group = val.groupby('Node')\nval_standard = val.copy()\n\nval_standard['Value'] = val_standard['Value'].astype(float)\n# 标准化\nfor cat,group in val_group:\n    means = group['Value'].mean()\n    stds = group['Value'].std()\n    val_standard.loc[group.index,'Value'] = (group['Value']-means)/stds","metadata":{"execution":{"iopub.status.busy":"2024-05-06T14:22:29.848601Z","iopub.execute_input":"2024-05-06T14:22:29.849305Z","iopub.status.idle":"2024-05-06T14:22:30.055678Z","shell.execute_reply.started":"2024-05-06T14:22:29.849273Z","shell.execute_reply":"2024-05-06T14:22:30.054920Z"},"trusted":true},"execution_count":76,"outputs":[]},{"cell_type":"code","source":"val_standard","metadata":{"execution":{"iopub.status.busy":"2024-05-06T14:22:32.681045Z","iopub.execute_input":"2024-05-06T14:22:32.681661Z","iopub.status.idle":"2024-05-06T14:22:32.697769Z","shell.execute_reply.started":"2024-05-06T14:22:32.681622Z","shell.execute_reply":"2024-05-06T14:22:32.696910Z"},"trusted":true},"execution_count":77,"outputs":[{"execution_count":77,"output_type":"execute_result","data":{"text/plain":"                     Date            Node     Value  Day_of_Week  \\\n0     2016-12-02 00:00:00           Total -0.569949            4   \n1     2016-12-02 00:00:00              de  1.006392            4   \n2     2016-12-02 00:00:00              en -0.145353            4   \n3     2016-12-02 00:00:00              fr  0.219679            4   \n4     2016-12-02 00:00:00              ja -0.589082            4   \n...                   ...             ...       ...          ...   \n2980  2016-12-16 00:00:00  zh_MOB_AAG_005  0.760371            4   \n2981  2016-12-16 00:00:00  zh_MOB_AAG_028 -0.286055            4   \n2982  2016-12-16 00:00:00  zh_MOB_AAG_031  3.590770            4   \n2983  2016-12-16 00:00:00  zh_MOB_AAG_036 -0.089982            4   \n2984  2016-12-16 00:00:00  zh_MOB_AAG_138 -0.009255            4   \n\n      Day_Of_Month  Month_Of_Year  Code  Agent  Access  State  \n0                2             12     0      0       0      0  \n1                2             12     0      0       0      1  \n2                2             12     0      0       0      2  \n3                2             12     0      0       0      3  \n4                2             12     0      0       0      4  \n...            ...            ...   ...    ...     ...    ...  \n2980            16             12   146     24      18      6  \n2981            16             12   147     24      18      6  \n2982            16             12   148     24      18      6  \n2983            16             12   149     24      18      6  \n2984            16             12   150     24      18      6  \n\n[2985 rows x 10 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Date</th>\n      <th>Node</th>\n      <th>Value</th>\n      <th>Day_of_Week</th>\n      <th>Day_Of_Month</th>\n      <th>Month_Of_Year</th>\n      <th>Code</th>\n      <th>Agent</th>\n      <th>Access</th>\n      <th>State</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2016-12-02 00:00:00</td>\n      <td>Total</td>\n      <td>-0.569949</td>\n      <td>4</td>\n      <td>2</td>\n      <td>12</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2016-12-02 00:00:00</td>\n      <td>de</td>\n      <td>1.006392</td>\n      <td>4</td>\n      <td>2</td>\n      <td>12</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2016-12-02 00:00:00</td>\n      <td>en</td>\n      <td>-0.145353</td>\n      <td>4</td>\n      <td>2</td>\n      <td>12</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2016-12-02 00:00:00</td>\n      <td>fr</td>\n      <td>0.219679</td>\n      <td>4</td>\n      <td>2</td>\n      <td>12</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2016-12-02 00:00:00</td>\n      <td>ja</td>\n      <td>-0.589082</td>\n      <td>4</td>\n      <td>2</td>\n      <td>12</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2980</th>\n      <td>2016-12-16 00:00:00</td>\n      <td>zh_MOB_AAG_005</td>\n      <td>0.760371</td>\n      <td>4</td>\n      <td>16</td>\n      <td>12</td>\n      <td>146</td>\n      <td>24</td>\n      <td>18</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>2981</th>\n      <td>2016-12-16 00:00:00</td>\n      <td>zh_MOB_AAG_028</td>\n      <td>-0.286055</td>\n      <td>4</td>\n      <td>16</td>\n      <td>12</td>\n      <td>147</td>\n      <td>24</td>\n      <td>18</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>2982</th>\n      <td>2016-12-16 00:00:00</td>\n      <td>zh_MOB_AAG_031</td>\n      <td>3.590770</td>\n      <td>4</td>\n      <td>16</td>\n      <td>12</td>\n      <td>148</td>\n      <td>24</td>\n      <td>18</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>2983</th>\n      <td>2016-12-16 00:00:00</td>\n      <td>zh_MOB_AAG_036</td>\n      <td>-0.089982</td>\n      <td>4</td>\n      <td>16</td>\n      <td>12</td>\n      <td>149</td>\n      <td>24</td>\n      <td>18</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>2984</th>\n      <td>2016-12-16 00:00:00</td>\n      <td>zh_MOB_AAG_138</td>\n      <td>-0.009255</td>\n      <td>4</td>\n      <td>16</td>\n      <td>12</td>\n      <td>150</td>\n      <td>24</td>\n      <td>18</td>\n      <td>6</td>\n    </tr>\n  </tbody>\n</table>\n<p>2985 rows × 10 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"val_ds = PandasDataset.from_long_dataframe(val_standard.iloc[:,[0,1,2,3,4,5]],\n                                             target=\"Value\",\n                                             timestamp='Date',\n                                             freq='D',\n                                             item_id=\"Node\",\n                                             feat_dynamic_real=['Day_of_Week','Day_Of_Month','Month_Of_Year'],\n                                             static_features=train_static)","metadata":{"execution":{"iopub.status.busy":"2024-05-06T14:22:43.560558Z","iopub.execute_input":"2024-05-06T14:22:43.560920Z","iopub.status.idle":"2024-05-06T14:22:43.584732Z","shell.execute_reply.started":"2024-05-06T14:22:43.560890Z","shell.execute_reply":"2024-05-06T14:22:43.583863Z"},"trusted":true},"execution_count":78,"outputs":[]},{"cell_type":"code","source":"from gluonts.torch.distributions import NormalOutput\nestimator = DeepAREstimator(\n            num_layers=3,\n            hidden_size=40,\n            lr=1e-3,\n            prediction_length=prediction_length,\n            context_length=10*prediction_length,\n            patience = 10,\n            freq='D',\n            distr_output=NormalOutput(),\n            scaling=False,\n            num_parallel_samples=1000,\n            batch_size=32,\n            trainer_kwargs={\n                \"max_epochs\": 1,\n            },\n        )\n\npredictor = estimator.train(train_ds,num_worker = 4)\nforecast_it = predictor.predict(val_ds)\n# forecast_it, ts_it = make_evaluation_predictions(val_ds, predictor=predictor,num_samples=1000)\nforecasts = list(forecast_it)","metadata":{"execution":{"iopub.status.busy":"2024-05-06T14:24:19.326727Z","iopub.execute_input":"2024-05-06T14:24:19.327389Z","iopub.status.idle":"2024-05-06T14:24:24.331416Z","shell.execute_reply.started":"2024-05-06T14:24:19.327351Z","shell.execute_reply":"2024-05-06T14:24:24.330617Z"},"trusted":true},"execution_count":83,"outputs":[{"name":"stderr","text":"INFO: GPU available: True (cuda), used: True\nINFO: TPU available: False, using: 0 TPU cores\nINFO: IPU available: False, using: 0 IPUs\nINFO: HPU available: False, using: 0 HPUs\n/opt/conda/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:74: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\nINFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\nINFO: \n  | Name  | Type        | Params | In sizes                                                         | Out sizes    \n-------------------------------------------------------------------------------------------------------------------------\n0 | model | DeepARModel | 39.0 K | [[1, 1], [1, 1], [1, 1242, 4], [1, 1242], [1, 1242], [1, 15, 4]] | [1, 1000, 15]\n-------------------------------------------------------------------------------------------------------------------------\n39.0 K    Trainable params\n0         Non-trainable params\n39.0 K    Total params\n0.156     Total estimated model params size (MB)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6b8872eb7db5409486675eb0e903363a"}},"metadata":{}},{"name":"stderr","text":"INFO: Epoch 0, global step 50: 'train_loss' reached 1.08094 (best 1.08094), saving model to '/kaggle/working/lightning_logs/version_11/checkpoints/epoch=0-step=50.ckpt' as top 1\nINFO: `Trainer.fit` stopped: `max_epochs=1` reached.\n","output_type":"stream"}]},{"cell_type":"code","source":"node_nonsort = []\nfor i in range(len(forecasts)):\n    node_nonsort.append(forecasts[i].item_id)\npath = '../input/wiki-data/'\nnew_df = pd.read_csv(path+'Wiki_process.csv')\nnode_list = new_df.columns[1:].tolist()","metadata":{"execution":{"iopub.status.busy":"2024-05-06T14:25:33.856751Z","iopub.execute_input":"2024-05-06T14:25:33.857098Z","iopub.status.idle":"2024-05-06T14:25:33.894348Z","shell.execute_reply.started":"2024-05-06T14:25:33.857071Z","shell.execute_reply":"2024-05-06T14:25:33.893467Z"},"trusted":true},"execution_count":85,"outputs":[]},{"cell_type":"code","source":"index_list = [node_nonsort.index(i) for i in node_list]","metadata":{"execution":{"iopub.status.busy":"2024-05-06T14:25:35.794395Z","iopub.execute_input":"2024-05-06T14:25:35.794804Z","iopub.status.idle":"2024-05-06T14:25:35.799803Z","shell.execute_reply.started":"2024-05-06T14:25:35.794768Z","shell.execute_reply":"2024-05-06T14:25:35.799071Z"},"trusted":true},"execution_count":86,"outputs":[]},{"cell_type":"code","source":"val_standard.head()","metadata":{"execution":{"iopub.status.busy":"2024-05-06T14:08:01.172482Z","iopub.execute_input":"2024-05-06T14:08:01.173238Z","iopub.status.idle":"2024-05-06T14:08:01.185807Z","shell.execute_reply.started":"2024-05-06T14:08:01.173203Z","shell.execute_reply":"2024-05-06T14:08:01.184813Z"},"trusted":true},"execution_count":46,"outputs":[{"execution_count":46,"output_type":"execute_result","data":{"text/plain":"                  Date   Node     Value  Day_of_Week  Day_Of_Month  \\\n0  2016-12-13 00:00:00  Total  0.006786            1            13   \n1  2016-12-13 00:00:00     de  0.177168            1            13   \n2  2016-12-13 00:00:00     en -0.416581            1            13   \n3  2016-12-13 00:00:00     fr  1.093462            1            13   \n4  2016-12-13 00:00:00     ja -0.045340            1            13   \n\n   Month_Of_Year  Code  Agent  Access  State  \n0             12     0      0       0      0  \n1             12     0      0       0      1  \n2             12     0      0       0      2  \n3             12     0      0       0      3  \n4             12     0      0       0      4  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Date</th>\n      <th>Node</th>\n      <th>Value</th>\n      <th>Day_of_Week</th>\n      <th>Day_Of_Month</th>\n      <th>Month_Of_Year</th>\n      <th>Code</th>\n      <th>Agent</th>\n      <th>Access</th>\n      <th>State</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2016-12-13 00:00:00</td>\n      <td>Total</td>\n      <td>0.006786</td>\n      <td>1</td>\n      <td>13</td>\n      <td>12</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2016-12-13 00:00:00</td>\n      <td>de</td>\n      <td>0.177168</td>\n      <td>1</td>\n      <td>13</td>\n      <td>12</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2016-12-13 00:00:00</td>\n      <td>en</td>\n      <td>-0.416581</td>\n      <td>1</td>\n      <td>13</td>\n      <td>12</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2016-12-13 00:00:00</td>\n      <td>fr</td>\n      <td>1.093462</td>\n      <td>1</td>\n      <td>13</td>\n      <td>12</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2016-12-13 00:00:00</td>\n      <td>ja</td>\n      <td>-0.045340</td>\n      <td>1</td>\n      <td>13</td>\n      <td>12</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>4</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"class DeepARTuningObjective:\n    def __init__(\n        self, dataset, prediction_length, freq, validation_label,val_ds,index_list\n    ):\n        self.dataset = dataset\n        self.prediction_length = prediction_length\n        self.freq = freq\n        self.validation_label=validation_label\n        self.val_ds = val_ds\n\n\n    def get_params(self, trial) -> dict:\n        return {\n            \"num_layers\": trial.suggest_int(\"num_layers\", 2, 5),\n            \"hidden_size\": trial.suggest_int(\"hidden_size\", 20, 50),\n            \"lr\":trial.suggest_categorical('lr', choices=[1e-4,1e-3,1e-2]),\n            \"batch_size\": trial.suggest_int(\"batch_size\", low= 16, high =64, step =16)\n        }\n\n    def __call__(self, trial):\n        params = self.get_params(trial)\n        estimator = DeepAREstimator(\n            num_layers=params[\"num_layers\"],\n            hidden_size=params[\"hidden_size\"],\n            lr=params[\"lr\"],\n            prediction_length=self.prediction_length,\n            context_length=10*prediction_length,\n            patience = 10,\n            freq=self.freq,\n            distr_output=NormalOutput(),\n            scaling=False,\n            num_parallel_samples=1000,\n            batch_size=params[\"batch_size\"],\n            trainer_kwargs={\n                \"max_epochs\": 20,\n            },\n        )\n\n        predictor = estimator.train(self.dataset, cache_data=True)\n        forecast_it, ts_it = make_evaluation_predictions(dataset=self.val_ds, predictor=predictor,num_samples=1000)\n\n        forecasts = list(forecast_it)\n        \n        samples_total = []\n        for j in range(15):\n            samples_step = []\n            for i in range(len(forecasts)):\n                samples_step.append(forecasts[i].samples[:,j].reshape((1,1000)))\n            samples_1 = []\n            for k in index_list:\n                samples_1.append(samples_step[k])\n            samples_total.append(np.concatenate(samples_1,axis=0))\n        samples_new = np.concatenate(samples_total,axis=1)\n        \n        real = []\n        for i in range(15):\n            real_step = np.array(val_standard.iloc[i*199:(i+1)*199,2].tolist())\n            repeated_arr = np.repeat(real_step, 1000, axis=0)\n            two_dim_array = repeated_arr.reshape(199, 1000).transpose()\n            real.append(two_dim_array)\n        real_2 = np.concatenate(real,axis=0).transpose()\n        \n        samples_new2 = samples_new.copy()\n        samples_new2 = samples_new2.transpose()\n        np.random.shuffle(samples_new2)\n        samples_new2 = samples_new2.transpose()\n        \n        dif1 = samples_new - samples_new2\n        dif2 = real_2 - samples_new\n        term1 = np.sum(np.square(dif1))\n        term2 = np.sum(np.square(dif2))\n        res1 = ((-0.5*term1)+term2)/(real_2.shape[1])\n        \n        return res1","metadata":{"execution":{"iopub.status.busy":"2024-05-06T14:26:15.485779Z","iopub.execute_input":"2024-05-06T14:26:15.486257Z","iopub.status.idle":"2024-05-06T14:26:15.503298Z","shell.execute_reply.started":"2024-05-06T14:26:15.486223Z","shell.execute_reply":"2024-05-06T14:26:15.502347Z"},"trusted":true},"execution_count":87,"outputs":[]},{"cell_type":"code","source":"import time\nimport optuna\n\nstart_time = time.time()\nstudy = optuna.create_study(direction=\"minimize\")\nstudy.optimize(\n    DeepARTuningObjective(\n        train_ds, prediction_length, freq,val_standard, val_ds, index_list\n    ),\n    n_trials=10,\n)\n\nprint(\"Number of finished trials: {}\".format(len(study.trials)))\n\nprint(\"Best trial:\")\ntrial = study.best_trial\n\nprint(\"  Value: {}\".format(trial.value))\n\nprint(\"  Params: \")\nfor key, value in trial.params.items():\n    print(\"    {}: {}\".format(key, value))\nprint(time.time() - start_time)","metadata":{"execution":{"iopub.status.busy":"2024-05-06T14:26:17.809796Z","iopub.execute_input":"2024-05-06T14:26:17.810269Z","iopub.status.idle":"2024-05-06T14:30:18.330906Z","shell.execute_reply.started":"2024-05-06T14:26:17.810236Z","shell.execute_reply":"2024-05-06T14:30:18.330033Z"},"trusted":true},"execution_count":88,"outputs":[{"name":"stderr","text":"[I 2024-05-06 14:26:17,813] A new study created in memory with name: no-name-b79122ef-accb-4467-8b34-8ce3cfbe37d4\nINFO: GPU available: True (cuda), used: True\nINFO: TPU available: False, using: 0 TPU cores\nINFO: IPU available: False, using: 0 IPUs\nINFO: HPU available: False, using: 0 HPUs\n/opt/conda/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:74: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\nINFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\nINFO: \n  | Name  | Type        | Params | In sizes                                                         | Out sizes    \n-------------------------------------------------------------------------------------------------------------------------\n0 | model | DeepARModel | 35.5 K | [[1, 1], [1, 1], [1, 1242, 4], [1, 1242], [1, 1242], [1, 15, 4]] | [1, 1000, 15]\n-------------------------------------------------------------------------------------------------------------------------\n35.5 K    Trainable params\n0         Non-trainable params\n35.5 K    Total params\n0.142     Total estimated model params size (MB)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9fe1d70656a9456fb6de39a3bb21ac83"}},"metadata":{}},{"name":"stderr","text":"INFO: Epoch 0, global step 50: 'train_loss' reached 1.03214 (best 1.03214), saving model to '/kaggle/working/lightning_logs/version_12/checkpoints/epoch=0-step=50.ckpt' as top 1\nINFO: Epoch 1, global step 100: 'train_loss' reached 0.91387 (best 0.91387), saving model to '/kaggle/working/lightning_logs/version_12/checkpoints/epoch=1-step=100.ckpt' as top 1\nINFO: Epoch 2, global step 150: 'train_loss' reached 0.85859 (best 0.85859), saving model to '/kaggle/working/lightning_logs/version_12/checkpoints/epoch=2-step=150.ckpt' as top 1\nINFO: Epoch 3, global step 200: 'train_loss' reached 0.81463 (best 0.81463), saving model to '/kaggle/working/lightning_logs/version_12/checkpoints/epoch=3-step=200.ckpt' as top 1\nINFO: Epoch 4, global step 250: 'train_loss' reached 0.77626 (best 0.77626), saving model to '/kaggle/working/lightning_logs/version_12/checkpoints/epoch=4-step=250.ckpt' as top 1\nINFO: Epoch 5, global step 300: 'train_loss' reached 0.73552 (best 0.73552), saving model to '/kaggle/working/lightning_logs/version_12/checkpoints/epoch=5-step=300.ckpt' as top 1\nINFO: Epoch 6, global step 350: 'train_loss' reached 0.70221 (best 0.70221), saving model to '/kaggle/working/lightning_logs/version_12/checkpoints/epoch=6-step=350.ckpt' as top 1\nINFO: Epoch 7, global step 400: 'train_loss' was not in top 1\nINFO: Epoch 8, global step 450: 'train_loss' reached 0.68150 (best 0.68150), saving model to '/kaggle/working/lightning_logs/version_12/checkpoints/epoch=8-step=450.ckpt' as top 1\nINFO: Epoch 9, global step 500: 'train_loss' reached 0.67172 (best 0.67172), saving model to '/kaggle/working/lightning_logs/version_12/checkpoints/epoch=9-step=500.ckpt' as top 1\nINFO: Epoch 10, global step 550: 'train_loss' reached 0.63606 (best 0.63606), saving model to '/kaggle/working/lightning_logs/version_12/checkpoints/epoch=10-step=550.ckpt' as top 1\nINFO: Epoch 11, global step 600: 'train_loss' reached 0.61159 (best 0.61159), saving model to '/kaggle/working/lightning_logs/version_12/checkpoints/epoch=11-step=600.ckpt' as top 1\nINFO: Epoch 12, global step 650: 'train_loss' was not in top 1\nINFO: Epoch 13, global step 700: 'train_loss' reached 0.57164 (best 0.57164), saving model to '/kaggle/working/lightning_logs/version_12/checkpoints/epoch=13-step=700.ckpt' as top 1\nINFO: Epoch 14, global step 750: 'train_loss' reached 0.55665 (best 0.55665), saving model to '/kaggle/working/lightning_logs/version_12/checkpoints/epoch=14-step=750.ckpt' as top 1\nINFO: Epoch 15, global step 800: 'train_loss' reached 0.55498 (best 0.55498), saving model to '/kaggle/working/lightning_logs/version_12/checkpoints/epoch=15-step=800.ckpt' as top 1\nINFO: Epoch 16, global step 850: 'train_loss' reached 0.47951 (best 0.47951), saving model to '/kaggle/working/lightning_logs/version_12/checkpoints/epoch=16-step=850.ckpt' as top 1\nINFO: Epoch 17, global step 900: 'train_loss' was not in top 1\nINFO: Epoch 18, global step 950: 'train_loss' reached 0.43258 (best 0.43258), saving model to '/kaggle/working/lightning_logs/version_12/checkpoints/epoch=18-step=950.ckpt' as top 1\nINFO: Epoch 19, global step 1000: 'train_loss' reached 0.40766 (best 0.40766), saving model to '/kaggle/working/lightning_logs/version_12/checkpoints/epoch=19-step=1000.ckpt' as top 1\nINFO: `Trainer.fit` stopped: `max_epochs=20` reached.\n[I 2024-05-06 14:26:41,525] Trial 0 finished with value: 206.70448379628 and parameters: {'num_layers': 3, 'hidden_size': 38, 'lr': 0.001, 'batch_size': 48}. Best is trial 0 with value: 206.70448379628.\nINFO: GPU available: True (cuda), used: True\nINFO: TPU available: False, using: 0 TPU cores\nINFO: IPU available: False, using: 0 IPUs\nINFO: HPU available: False, using: 0 HPUs\n/opt/conda/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:74: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\nINFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\nINFO: \n  | Name  | Type        | Params | In sizes                                                         | Out sizes    \n-------------------------------------------------------------------------------------------------------------------------\n0 | model | DeepARModel | 13.5 K | [[1, 1], [1, 1], [1, 1242, 4], [1, 1242], [1, 1242], [1, 15, 4]] | [1, 1000, 15]\n-------------------------------------------------------------------------------------------------------------------------\n13.5 K    Trainable params\n0         Non-trainable params\n13.5 K    Total params\n0.054     Total estimated model params size (MB)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c3530b129277481d8948d7d03f4225bc"}},"metadata":{}},{"name":"stderr","text":"INFO: Epoch 0, global step 50: 'train_loss' reached 1.23902 (best 1.23902), saving model to '/kaggle/working/lightning_logs/version_13/checkpoints/epoch=0-step=50.ckpt' as top 1\nINFO: Epoch 1, global step 100: 'train_loss' reached 1.17436 (best 1.17436), saving model to '/kaggle/working/lightning_logs/version_13/checkpoints/epoch=1-step=100.ckpt' as top 1\nINFO: Epoch 2, global step 150: 'train_loss' reached 1.16375 (best 1.16375), saving model to '/kaggle/working/lightning_logs/version_13/checkpoints/epoch=2-step=150.ckpt' as top 1\nINFO: Epoch 3, global step 200: 'train_loss' reached 1.08552 (best 1.08552), saving model to '/kaggle/working/lightning_logs/version_13/checkpoints/epoch=3-step=200.ckpt' as top 1\nINFO: Epoch 4, global step 250: 'train_loss' reached 1.04809 (best 1.04809), saving model to '/kaggle/working/lightning_logs/version_13/checkpoints/epoch=4-step=250.ckpt' as top 1\nINFO: Epoch 5, global step 300: 'train_loss' reached 1.04557 (best 1.04557), saving model to '/kaggle/working/lightning_logs/version_13/checkpoints/epoch=5-step=300.ckpt' as top 1\nINFO: Epoch 6, global step 350: 'train_loss' reached 1.02299 (best 1.02299), saving model to '/kaggle/working/lightning_logs/version_13/checkpoints/epoch=6-step=350.ckpt' as top 1\nINFO: Epoch 7, global step 400: 'train_loss' reached 1.00423 (best 1.00423), saving model to '/kaggle/working/lightning_logs/version_13/checkpoints/epoch=7-step=400.ckpt' as top 1\nINFO: Epoch 8, global step 450: 'train_loss' reached 0.98277 (best 0.98277), saving model to '/kaggle/working/lightning_logs/version_13/checkpoints/epoch=8-step=450.ckpt' as top 1\nINFO: Epoch 9, global step 500: 'train_loss' reached 0.97437 (best 0.97437), saving model to '/kaggle/working/lightning_logs/version_13/checkpoints/epoch=9-step=500.ckpt' as top 1\nINFO: Epoch 10, global step 550: 'train_loss' was not in top 1\nINFO: Epoch 11, global step 600: 'train_loss' reached 0.97102 (best 0.97102), saving model to '/kaggle/working/lightning_logs/version_13/checkpoints/epoch=11-step=600.ckpt' as top 1\nINFO: Epoch 12, global step 650: 'train_loss' reached 0.94630 (best 0.94630), saving model to '/kaggle/working/lightning_logs/version_13/checkpoints/epoch=12-step=650.ckpt' as top 1\nINFO: Epoch 13, global step 700: 'train_loss' was not in top 1\nINFO: Epoch 14, global step 750: 'train_loss' reached 0.94345 (best 0.94345), saving model to '/kaggle/working/lightning_logs/version_13/checkpoints/epoch=14-step=750.ckpt' as top 1\nINFO: Epoch 15, global step 800: 'train_loss' was not in top 1\nINFO: Epoch 16, global step 850: 'train_loss' reached 0.93020 (best 0.93020), saving model to '/kaggle/working/lightning_logs/version_13/checkpoints/epoch=16-step=850.ckpt' as top 1\nINFO: Epoch 17, global step 900: 'train_loss' was not in top 1\nINFO: Epoch 18, global step 950: 'train_loss' reached 0.92492 (best 0.92492), saving model to '/kaggle/working/lightning_logs/version_13/checkpoints/epoch=18-step=950.ckpt' as top 1\nINFO: Epoch 19, global step 1000: 'train_loss' reached 0.91253 (best 0.91253), saving model to '/kaggle/working/lightning_logs/version_13/checkpoints/epoch=19-step=1000.ckpt' as top 1\nINFO: `Trainer.fit` stopped: `max_epochs=20` reached.\n[I 2024-05-06 14:27:08,595] Trial 1 finished with value: 279.2392427221061 and parameters: {'num_layers': 3, 'hidden_size': 22, 'lr': 0.0001, 'batch_size': 64}. Best is trial 0 with value: 206.70448379628.\nINFO: GPU available: True (cuda), used: True\nINFO: TPU available: False, using: 0 TPU cores\nINFO: IPU available: False, using: 0 IPUs\nINFO: HPU available: False, using: 0 HPUs\n/opt/conda/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:74: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\nINFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\nINFO: \n  | Name  | Type        | Params | In sizes                                                         | Out sizes    \n-------------------------------------------------------------------------------------------------------------------------\n0 | model | DeepARModel | 17.6 K | [[1, 1], [1, 1], [1, 1242, 4], [1, 1242], [1, 1242], [1, 15, 4]] | [1, 1000, 15]\n-------------------------------------------------------------------------------------------------------------------------\n17.6 K    Trainable params\n0         Non-trainable params\n17.6 K    Total params\n0.070     Total estimated model params size (MB)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"47cd55d77bfa4b80b73c1addc6a0f641"}},"metadata":{}},{"name":"stderr","text":"INFO: Epoch 0, global step 50: 'train_loss' reached 0.90025 (best 0.90025), saving model to '/kaggle/working/lightning_logs/version_14/checkpoints/epoch=0-step=50.ckpt' as top 1\nINFO: Epoch 1, global step 100: 'train_loss' reached 0.75986 (best 0.75986), saving model to '/kaggle/working/lightning_logs/version_14/checkpoints/epoch=1-step=100.ckpt' as top 1\nINFO: Epoch 2, global step 150: 'train_loss' reached 0.70548 (best 0.70548), saving model to '/kaggle/working/lightning_logs/version_14/checkpoints/epoch=2-step=150.ckpt' as top 1\nINFO: Epoch 3, global step 200: 'train_loss' reached 0.63127 (best 0.63127), saving model to '/kaggle/working/lightning_logs/version_14/checkpoints/epoch=3-step=200.ckpt' as top 1\nINFO: Epoch 4, global step 250: 'train_loss' reached 0.60704 (best 0.60704), saving model to '/kaggle/working/lightning_logs/version_14/checkpoints/epoch=4-step=250.ckpt' as top 1\nINFO: Epoch 5, global step 300: 'train_loss' reached 0.54573 (best 0.54573), saving model to '/kaggle/working/lightning_logs/version_14/checkpoints/epoch=5-step=300.ckpt' as top 1\nINFO: Epoch 6, global step 350: 'train_loss' reached 0.48196 (best 0.48196), saving model to '/kaggle/working/lightning_logs/version_14/checkpoints/epoch=6-step=350.ckpt' as top 1\nINFO: Epoch 7, global step 400: 'train_loss' reached 0.40887 (best 0.40887), saving model to '/kaggle/working/lightning_logs/version_14/checkpoints/epoch=7-step=400.ckpt' as top 1\nINFO: Epoch 8, global step 450: 'train_loss' reached 0.37725 (best 0.37725), saving model to '/kaggle/working/lightning_logs/version_14/checkpoints/epoch=8-step=450.ckpt' as top 1\nINFO: Epoch 9, global step 500: 'train_loss' was not in top 1\nINFO: Epoch 10, global step 550: 'train_loss' was not in top 1\nINFO: Epoch 11, global step 600: 'train_loss' reached 0.28121 (best 0.28121), saving model to '/kaggle/working/lightning_logs/version_14/checkpoints/epoch=11-step=600.ckpt' as top 1\nINFO: Epoch 12, global step 650: 'train_loss' reached 0.20075 (best 0.20075), saving model to '/kaggle/working/lightning_logs/version_14/checkpoints/epoch=12-step=650.ckpt' as top 1\nINFO: Epoch 13, global step 700: 'train_loss' was not in top 1\nINFO: Epoch 14, global step 750: 'train_loss' was not in top 1\nINFO: Epoch 15, global step 800: 'train_loss' was not in top 1\nINFO: Epoch 16, global step 850: 'train_loss' was not in top 1\nINFO: Epoch 17, global step 900: 'train_loss' was not in top 1\nINFO: Epoch 18, global step 950: 'train_loss' reached 0.12680 (best 0.12680), saving model to '/kaggle/working/lightning_logs/version_14/checkpoints/epoch=18-step=950.ckpt' as top 1\nINFO: Epoch 19, global step 1000: 'train_loss' reached 0.10132 (best 0.10132), saving model to '/kaggle/working/lightning_logs/version_14/checkpoints/epoch=19-step=1000.ckpt' as top 1\nINFO: `Trainer.fit` stopped: `max_epochs=20` reached.\n[I 2024-05-06 14:27:31,587] Trial 2 finished with value: 204.06890849803824 and parameters: {'num_layers': 2, 'hidden_size': 32, 'lr': 0.01, 'batch_size': 48}. Best is trial 2 with value: 204.06890849803824.\nINFO: GPU available: True (cuda), used: True\nINFO: TPU available: False, using: 0 TPU cores\nINFO: IPU available: False, using: 0 IPUs\nINFO: HPU available: False, using: 0 HPUs\n/opt/conda/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:74: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\nINFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\nINFO: \n  | Name  | Type        | Params | In sizes                                                         | Out sizes    \n-------------------------------------------------------------------------------------------------------------------------\n0 | model | DeepARModel | 21.9 K | [[1, 1], [1, 1], [1, 1242, 4], [1, 1242], [1, 1242], [1, 15, 4]] | [1, 1000, 15]\n-------------------------------------------------------------------------------------------------------------------------\n21.9 K    Trainable params\n0         Non-trainable params\n21.9 K    Total params\n0.087     Total estimated model params size (MB)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4981609b39484182bdd48a7cdb6b57dd"}},"metadata":{}},{"name":"stderr","text":"INFO: Epoch 0, global step 50: 'train_loss' reached 0.91642 (best 0.91642), saving model to '/kaggle/working/lightning_logs/version_15/checkpoints/epoch=0-step=50.ckpt' as top 1\nINFO: Epoch 1, global step 100: 'train_loss' reached 0.75579 (best 0.75579), saving model to '/kaggle/working/lightning_logs/version_15/checkpoints/epoch=1-step=100.ckpt' as top 1\nINFO: Epoch 2, global step 150: 'train_loss' reached 0.69251 (best 0.69251), saving model to '/kaggle/working/lightning_logs/version_15/checkpoints/epoch=2-step=150.ckpt' as top 1\nINFO: Epoch 3, global step 200: 'train_loss' reached 0.64864 (best 0.64864), saving model to '/kaggle/working/lightning_logs/version_15/checkpoints/epoch=3-step=200.ckpt' as top 1\nINFO: Epoch 4, global step 250: 'train_loss' reached 0.58732 (best 0.58732), saving model to '/kaggle/working/lightning_logs/version_15/checkpoints/epoch=4-step=250.ckpt' as top 1\nINFO: Epoch 5, global step 300: 'train_loss' reached 0.58360 (best 0.58360), saving model to '/kaggle/working/lightning_logs/version_15/checkpoints/epoch=5-step=300.ckpt' as top 1\nINFO: Epoch 6, global step 350: 'train_loss' reached 0.51609 (best 0.51609), saving model to '/kaggle/working/lightning_logs/version_15/checkpoints/epoch=6-step=350.ckpt' as top 1\nINFO: Epoch 7, global step 400: 'train_loss' was not in top 1\nINFO: Epoch 8, global step 450: 'train_loss' was not in top 1\nINFO: Epoch 9, global step 500: 'train_loss' was not in top 1\nINFO: Epoch 10, global step 550: 'train_loss' was not in top 1\nINFO: Epoch 11, global step 600: 'train_loss' was not in top 1\nINFO: Epoch 12, global step 650: 'train_loss' was not in top 1\nINFO: Epoch 13, global step 700: 'train_loss' reached 0.49723 (best 0.49723), saving model to '/kaggle/working/lightning_logs/version_15/checkpoints/epoch=13-step=700.ckpt' as top 1\nINFO: Epoch 14, global step 750: 'train_loss' reached 0.48123 (best 0.48123), saving model to '/kaggle/working/lightning_logs/version_15/checkpoints/epoch=14-step=750.ckpt' as top 1\nINFO: Epoch 15, global step 800: 'train_loss' was not in top 1\nINFO: Epoch 16, global step 850: 'train_loss' reached 0.44504 (best 0.44504), saving model to '/kaggle/working/lightning_logs/version_15/checkpoints/epoch=16-step=850.ckpt' as top 1\nINFO: Epoch 17, global step 900: 'train_loss' reached 0.39527 (best 0.39527), saving model to '/kaggle/working/lightning_logs/version_15/checkpoints/epoch=17-step=900.ckpt' as top 1\nINFO: Epoch 18, global step 950: 'train_loss' reached 0.38410 (best 0.38410), saving model to '/kaggle/working/lightning_logs/version_15/checkpoints/epoch=18-step=950.ckpt' as top 1\nINFO: Epoch 19, global step 1000: 'train_loss' reached 0.33470 (best 0.33470), saving model to '/kaggle/working/lightning_logs/version_15/checkpoints/epoch=19-step=1000.ckpt' as top 1\nINFO: `Trainer.fit` stopped: `max_epochs=20` reached.\n[I 2024-05-06 14:27:54,681] Trial 3 finished with value: 241.47062401303884 and parameters: {'num_layers': 3, 'hidden_size': 29, 'lr': 0.01, 'batch_size': 48}. Best is trial 2 with value: 204.06890849803824.\nINFO: GPU available: True (cuda), used: True\nINFO: TPU available: False, using: 0 TPU cores\nINFO: IPU available: False, using: 0 IPUs\nINFO: HPU available: False, using: 0 HPUs\n/opt/conda/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:74: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\nINFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\nINFO: \n  | Name  | Type        | Params | In sizes                                                         | Out sizes    \n-------------------------------------------------------------------------------------------------------------------------\n0 | model | DeepARModel | 38.6 K | [[1, 1], [1, 1], [1, 1242, 4], [1, 1242], [1, 1242], [1, 15, 4]] | [1, 1000, 15]\n-------------------------------------------------------------------------------------------------------------------------\n38.6 K    Trainable params\n0         Non-trainable params\n38.6 K    Total params\n0.154     Total estimated model params size (MB)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aece0d24fbb14bd59d5b2ce7574670ad"}},"metadata":{}},{"name":"stderr","text":"INFO: Epoch 0, global step 50: 'train_loss' reached 1.10028 (best 1.10028), saving model to '/kaggle/working/lightning_logs/version_16/checkpoints/epoch=0-step=50.ckpt' as top 1\nINFO: Epoch 1, global step 100: 'train_loss' reached 0.97903 (best 0.97903), saving model to '/kaggle/working/lightning_logs/version_16/checkpoints/epoch=1-step=100.ckpt' as top 1\nINFO: Epoch 2, global step 150: 'train_loss' reached 0.93963 (best 0.93963), saving model to '/kaggle/working/lightning_logs/version_16/checkpoints/epoch=2-step=150.ckpt' as top 1\nINFO: Epoch 3, global step 200: 'train_loss' reached 0.85703 (best 0.85703), saving model to '/kaggle/working/lightning_logs/version_16/checkpoints/epoch=3-step=200.ckpt' as top 1\nINFO: Epoch 4, global step 250: 'train_loss' reached 0.81138 (best 0.81138), saving model to '/kaggle/working/lightning_logs/version_16/checkpoints/epoch=4-step=250.ckpt' as top 1\nINFO: Epoch 5, global step 300: 'train_loss' reached 0.76695 (best 0.76695), saving model to '/kaggle/working/lightning_logs/version_16/checkpoints/epoch=5-step=300.ckpt' as top 1\nINFO: Epoch 6, global step 350: 'train_loss' reached 0.71559 (best 0.71559), saving model to '/kaggle/working/lightning_logs/version_16/checkpoints/epoch=6-step=350.ckpt' as top 1\nINFO: Epoch 7, global step 400: 'train_loss' reached 0.68749 (best 0.68749), saving model to '/kaggle/working/lightning_logs/version_16/checkpoints/epoch=7-step=400.ckpt' as top 1\nINFO: Epoch 8, global step 450: 'train_loss' reached 0.66804 (best 0.66804), saving model to '/kaggle/working/lightning_logs/version_16/checkpoints/epoch=8-step=450.ckpt' as top 1\nINFO: Epoch 9, global step 500: 'train_loss' reached 0.65127 (best 0.65127), saving model to '/kaggle/working/lightning_logs/version_16/checkpoints/epoch=9-step=500.ckpt' as top 1\nINFO: Epoch 10, global step 550: 'train_loss' reached 0.63254 (best 0.63254), saving model to '/kaggle/working/lightning_logs/version_16/checkpoints/epoch=10-step=550.ckpt' as top 1\nINFO: Epoch 11, global step 600: 'train_loss' reached 0.60029 (best 0.60029), saving model to '/kaggle/working/lightning_logs/version_16/checkpoints/epoch=11-step=600.ckpt' as top 1\nINFO: Epoch 12, global step 650: 'train_loss' was not in top 1\nINFO: Epoch 13, global step 700: 'train_loss' reached 0.57168 (best 0.57168), saving model to '/kaggle/working/lightning_logs/version_16/checkpoints/epoch=13-step=700.ckpt' as top 1\nINFO: Epoch 14, global step 750: 'train_loss' reached 0.54937 (best 0.54937), saving model to '/kaggle/working/lightning_logs/version_16/checkpoints/epoch=14-step=750.ckpt' as top 1\nINFO: Epoch 15, global step 800: 'train_loss' reached 0.54265 (best 0.54265), saving model to '/kaggle/working/lightning_logs/version_16/checkpoints/epoch=15-step=800.ckpt' as top 1\nINFO: Epoch 16, global step 850: 'train_loss' reached 0.51778 (best 0.51778), saving model to '/kaggle/working/lightning_logs/version_16/checkpoints/epoch=16-step=850.ckpt' as top 1\nINFO: Epoch 17, global step 900: 'train_loss' reached 0.48870 (best 0.48870), saving model to '/kaggle/working/lightning_logs/version_16/checkpoints/epoch=17-step=900.ckpt' as top 1\nINFO: Epoch 18, global step 950: 'train_loss' reached 0.45644 (best 0.45644), saving model to '/kaggle/working/lightning_logs/version_16/checkpoints/epoch=18-step=950.ckpt' as top 1\nINFO: Epoch 19, global step 1000: 'train_loss' reached 0.43039 (best 0.43039), saving model to '/kaggle/working/lightning_logs/version_16/checkpoints/epoch=19-step=1000.ckpt' as top 1\nINFO: `Trainer.fit` stopped: `max_epochs=20` reached.\n[I 2024-05-06 14:28:22,944] Trial 4 finished with value: 248.31018139954458 and parameters: {'num_layers': 4, 'hidden_size': 34, 'lr': 0.001, 'batch_size': 64}. Best is trial 2 with value: 204.06890849803824.\nINFO: GPU available: True (cuda), used: True\nINFO: TPU available: False, using: 0 TPU cores\nINFO: IPU available: False, using: 0 IPUs\nINFO: HPU available: False, using: 0 HPUs\n/opt/conda/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:74: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\nINFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\nINFO: \n  | Name  | Type        | Params | In sizes                                                         | Out sizes    \n-------------------------------------------------------------------------------------------------------------------------\n0 | model | DeepARModel | 40.8 K | [[1, 1], [1, 1], [1, 1242, 4], [1, 1242], [1, 1242], [1, 15, 4]] | [1, 1000, 15]\n-------------------------------------------------------------------------------------------------------------------------\n40.8 K    Trainable params\n0         Non-trainable params\n40.8 K    Total params\n0.163     Total estimated model params size (MB)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"84228ec361ba477084aa6fac42766d5d"}},"metadata":{}},{"name":"stderr","text":"INFO: Epoch 0, global step 50: 'train_loss' reached 1.13139 (best 1.13139), saving model to '/kaggle/working/lightning_logs/version_17/checkpoints/epoch=0-step=50.ckpt' as top 1\nINFO: Epoch 1, global step 100: 'train_loss' reached 1.10936 (best 1.10936), saving model to '/kaggle/working/lightning_logs/version_17/checkpoints/epoch=1-step=100.ckpt' as top 1\nINFO: Epoch 2, global step 150: 'train_loss' reached 1.06065 (best 1.06065), saving model to '/kaggle/working/lightning_logs/version_17/checkpoints/epoch=2-step=150.ckpt' as top 1\nINFO: Epoch 3, global step 200: 'train_loss' reached 1.03736 (best 1.03736), saving model to '/kaggle/working/lightning_logs/version_17/checkpoints/epoch=3-step=200.ckpt' as top 1\nINFO: Epoch 4, global step 250: 'train_loss' reached 1.00554 (best 1.00554), saving model to '/kaggle/working/lightning_logs/version_17/checkpoints/epoch=4-step=250.ckpt' as top 1\nINFO: Epoch 5, global step 300: 'train_loss' reached 0.97891 (best 0.97891), saving model to '/kaggle/working/lightning_logs/version_17/checkpoints/epoch=5-step=300.ckpt' as top 1\nINFO: Epoch 6, global step 350: 'train_loss' reached 0.97729 (best 0.97729), saving model to '/kaggle/working/lightning_logs/version_17/checkpoints/epoch=6-step=350.ckpt' as top 1\nINFO: Epoch 7, global step 400: 'train_loss' reached 0.96000 (best 0.96000), saving model to '/kaggle/working/lightning_logs/version_17/checkpoints/epoch=7-step=400.ckpt' as top 1\nINFO: Epoch 8, global step 450: 'train_loss' reached 0.93486 (best 0.93486), saving model to '/kaggle/working/lightning_logs/version_17/checkpoints/epoch=8-step=450.ckpt' as top 1\nINFO: Epoch 9, global step 500: 'train_loss' reached 0.92478 (best 0.92478), saving model to '/kaggle/working/lightning_logs/version_17/checkpoints/epoch=9-step=500.ckpt' as top 1\nINFO: Epoch 10, global step 550: 'train_loss' reached 0.91350 (best 0.91350), saving model to '/kaggle/working/lightning_logs/version_17/checkpoints/epoch=10-step=550.ckpt' as top 1\nINFO: Epoch 11, global step 600: 'train_loss' reached 0.89243 (best 0.89243), saving model to '/kaggle/working/lightning_logs/version_17/checkpoints/epoch=11-step=600.ckpt' as top 1\nINFO: Epoch 12, global step 650: 'train_loss' was not in top 1\nINFO: Epoch 13, global step 700: 'train_loss' was not in top 1\nINFO: Epoch 14, global step 750: 'train_loss' was not in top 1\nINFO: Epoch 15, global step 800: 'train_loss' reached 0.87710 (best 0.87710), saving model to '/kaggle/working/lightning_logs/version_17/checkpoints/epoch=15-step=800.ckpt' as top 1\nINFO: Epoch 16, global step 850: 'train_loss' was not in top 1\nINFO: Epoch 17, global step 900: 'train_loss' reached 0.86604 (best 0.86604), saving model to '/kaggle/working/lightning_logs/version_17/checkpoints/epoch=17-step=900.ckpt' as top 1\nINFO: Epoch 18, global step 950: 'train_loss' reached 0.85239 (best 0.85239), saving model to '/kaggle/working/lightning_logs/version_17/checkpoints/epoch=18-step=950.ckpt' as top 1\nINFO: Epoch 19, global step 1000: 'train_loss' reached 0.82994 (best 0.82994), saving model to '/kaggle/working/lightning_logs/version_17/checkpoints/epoch=19-step=1000.ckpt' as top 1\nINFO: `Trainer.fit` stopped: `max_epochs=20` reached.\n[I 2024-05-06 14:28:46,502] Trial 5 finished with value: 212.0172464745319 and parameters: {'num_layers': 3, 'hidden_size': 41, 'lr': 0.0001, 'batch_size': 48}. Best is trial 2 with value: 204.06890849803824.\nINFO: GPU available: True (cuda), used: True\nINFO: TPU available: False, using: 0 TPU cores\nINFO: IPU available: False, using: 0 IPUs\nINFO: HPU available: False, using: 0 HPUs\n/opt/conda/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:74: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\nINFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\nINFO: \n  | Name  | Type        | Params | In sizes                                                         | Out sizes    \n-------------------------------------------------------------------------------------------------------------------------\n0 | model | DeepARModel | 30.5 K | [[1, 1], [1, 1], [1, 1242, 4], [1, 1242], [1, 1242], [1, 15, 4]] | [1, 1000, 15]\n-------------------------------------------------------------------------------------------------------------------------\n30.5 K    Trainable params\n0         Non-trainable params\n30.5 K    Total params\n0.122     Total estimated model params size (MB)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9f86f8a9520248af8b530d0bcdb7af47"}},"metadata":{}},{"name":"stderr","text":"INFO: Epoch 0, global step 50: 'train_loss' reached 0.95556 (best 0.95556), saving model to '/kaggle/working/lightning_logs/version_18/checkpoints/epoch=0-step=50.ckpt' as top 1\nINFO: Epoch 1, global step 100: 'train_loss' reached 0.77930 (best 0.77930), saving model to '/kaggle/working/lightning_logs/version_18/checkpoints/epoch=1-step=100.ckpt' as top 1\nINFO: Epoch 2, global step 150: 'train_loss' reached 0.77158 (best 0.77158), saving model to '/kaggle/working/lightning_logs/version_18/checkpoints/epoch=2-step=150.ckpt' as top 1\nINFO: Epoch 3, global step 200: 'train_loss' reached 0.74520 (best 0.74520), saving model to '/kaggle/working/lightning_logs/version_18/checkpoints/epoch=3-step=200.ckpt' as top 1\nINFO: Epoch 4, global step 250: 'train_loss' reached 0.71508 (best 0.71508), saving model to '/kaggle/working/lightning_logs/version_18/checkpoints/epoch=4-step=250.ckpt' as top 1\nINFO: Epoch 5, global step 300: 'train_loss' reached 0.66197 (best 0.66197), saving model to '/kaggle/working/lightning_logs/version_18/checkpoints/epoch=5-step=300.ckpt' as top 1\nINFO: Epoch 6, global step 350: 'train_loss' reached 0.64821 (best 0.64821), saving model to '/kaggle/working/lightning_logs/version_18/checkpoints/epoch=6-step=350.ckpt' as top 1\nINFO: Epoch 7, global step 400: 'train_loss' was not in top 1\nINFO: Epoch 8, global step 450: 'train_loss' was not in top 1\nINFO: Epoch 9, global step 500: 'train_loss' reached 0.60626 (best 0.60626), saving model to '/kaggle/working/lightning_logs/version_18/checkpoints/epoch=9-step=500.ckpt' as top 1\nINFO: Epoch 10, global step 550: 'train_loss' reached 0.58533 (best 0.58533), saving model to '/kaggle/working/lightning_logs/version_18/checkpoints/epoch=10-step=550.ckpt' as top 1\nINFO: Epoch 11, global step 600: 'train_loss' reached 0.53124 (best 0.53124), saving model to '/kaggle/working/lightning_logs/version_18/checkpoints/epoch=11-step=600.ckpt' as top 1\nINFO: Epoch 12, global step 650: 'train_loss' was not in top 1\nINFO: Epoch 13, global step 700: 'train_loss' was not in top 1\nINFO: Epoch 14, global step 750: 'train_loss' reached 0.52631 (best 0.52631), saving model to '/kaggle/working/lightning_logs/version_18/checkpoints/epoch=14-step=750.ckpt' as top 1\nINFO: Epoch 15, global step 800: 'train_loss' reached 0.52106 (best 0.52106), saving model to '/kaggle/working/lightning_logs/version_18/checkpoints/epoch=15-step=800.ckpt' as top 1\nINFO: Epoch 16, global step 850: 'train_loss' reached 0.44912 (best 0.44912), saving model to '/kaggle/working/lightning_logs/version_18/checkpoints/epoch=16-step=850.ckpt' as top 1\nINFO: Epoch 17, global step 900: 'train_loss' was not in top 1\nINFO: Epoch 18, global step 950: 'train_loss' reached 0.37300 (best 0.37300), saving model to '/kaggle/working/lightning_logs/version_18/checkpoints/epoch=18-step=950.ckpt' as top 1\nINFO: Epoch 19, global step 1000: 'train_loss' reached 0.32092 (best 0.32092), saving model to '/kaggle/working/lightning_logs/version_18/checkpoints/epoch=19-step=1000.ckpt' as top 1\nINFO: `Trainer.fit` stopped: `max_epochs=20` reached.\n[I 2024-05-06 14:29:02,405] Trial 6 finished with value: 189.0260339682677 and parameters: {'num_layers': 2, 'hidden_size': 44, 'lr': 0.01, 'batch_size': 16}. Best is trial 6 with value: 189.0260339682677.\nINFO: GPU available: True (cuda), used: True\nINFO: TPU available: False, using: 0 TPU cores\nINFO: IPU available: False, using: 0 IPUs\nINFO: HPU available: False, using: 0 HPUs\n/opt/conda/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:74: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\nINFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\nINFO: \n  | Name  | Type        | Params | In sizes                                                         | Out sizes    \n-------------------------------------------------------------------------------------------------------------------------\n0 | model | DeepARModel | 16.9 K | [[1, 1], [1, 1], [1, 1242, 4], [1, 1242], [1, 1242], [1, 15, 4]] | [1, 1000, 15]\n-------------------------------------------------------------------------------------------------------------------------\n16.9 K    Trainable params\n0         Non-trainable params\n16.9 K    Total params\n0.067     Total estimated model params size (MB)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"17a9275a2ec54b41bafcf3dd0d3d0c91"}},"metadata":{}},{"name":"stderr","text":"INFO: Epoch 0, global step 50: 'train_loss' reached 1.04993 (best 1.04993), saving model to '/kaggle/working/lightning_logs/version_19/checkpoints/epoch=0-step=50.ckpt' as top 1\nINFO: Epoch 1, global step 100: 'train_loss' reached 0.94289 (best 0.94289), saving model to '/kaggle/working/lightning_logs/version_19/checkpoints/epoch=1-step=100.ckpt' as top 1\nINFO: Epoch 2, global step 150: 'train_loss' reached 0.88566 (best 0.88566), saving model to '/kaggle/working/lightning_logs/version_19/checkpoints/epoch=2-step=150.ckpt' as top 1\nINFO: Epoch 3, global step 200: 'train_loss' reached 0.84738 (best 0.84738), saving model to '/kaggle/working/lightning_logs/version_19/checkpoints/epoch=3-step=200.ckpt' as top 1\nINFO: Epoch 4, global step 250: 'train_loss' reached 0.81007 (best 0.81007), saving model to '/kaggle/working/lightning_logs/version_19/checkpoints/epoch=4-step=250.ckpt' as top 1\nINFO: Epoch 5, global step 300: 'train_loss' reached 0.75954 (best 0.75954), saving model to '/kaggle/working/lightning_logs/version_19/checkpoints/epoch=5-step=300.ckpt' as top 1\nINFO: Epoch 6, global step 350: 'train_loss' reached 0.72861 (best 0.72861), saving model to '/kaggle/working/lightning_logs/version_19/checkpoints/epoch=6-step=350.ckpt' as top 1\nINFO: Epoch 7, global step 400: 'train_loss' reached 0.71552 (best 0.71552), saving model to '/kaggle/working/lightning_logs/version_19/checkpoints/epoch=7-step=400.ckpt' as top 1\nINFO: Epoch 8, global step 450: 'train_loss' reached 0.68034 (best 0.68034), saving model to '/kaggle/working/lightning_logs/version_19/checkpoints/epoch=8-step=450.ckpt' as top 1\nINFO: Epoch 9, global step 500: 'train_loss' reached 0.64960 (best 0.64960), saving model to '/kaggle/working/lightning_logs/version_19/checkpoints/epoch=9-step=500.ckpt' as top 1\nINFO: Epoch 10, global step 550: 'train_loss' reached 0.62896 (best 0.62896), saving model to '/kaggle/working/lightning_logs/version_19/checkpoints/epoch=10-step=550.ckpt' as top 1\nINFO: Epoch 11, global step 600: 'train_loss' reached 0.61139 (best 0.61139), saving model to '/kaggle/working/lightning_logs/version_19/checkpoints/epoch=11-step=600.ckpt' as top 1\nINFO: Epoch 12, global step 650: 'train_loss' reached 0.56926 (best 0.56926), saving model to '/kaggle/working/lightning_logs/version_19/checkpoints/epoch=12-step=650.ckpt' as top 1\nINFO: Epoch 13, global step 700: 'train_loss' reached 0.56511 (best 0.56511), saving model to '/kaggle/working/lightning_logs/version_19/checkpoints/epoch=13-step=700.ckpt' as top 1\nINFO: Epoch 14, global step 750: 'train_loss' reached 0.54821 (best 0.54821), saving model to '/kaggle/working/lightning_logs/version_19/checkpoints/epoch=14-step=750.ckpt' as top 1\nINFO: Epoch 15, global step 800: 'train_loss' reached 0.51085 (best 0.51085), saving model to '/kaggle/working/lightning_logs/version_19/checkpoints/epoch=15-step=800.ckpt' as top 1\nINFO: Epoch 16, global step 850: 'train_loss' reached 0.50974 (best 0.50974), saving model to '/kaggle/working/lightning_logs/version_19/checkpoints/epoch=16-step=850.ckpt' as top 1\nINFO: Epoch 17, global step 900: 'train_loss' was not in top 1\nINFO: Epoch 18, global step 950: 'train_loss' reached 0.49446 (best 0.49446), saving model to '/kaggle/working/lightning_logs/version_19/checkpoints/epoch=18-step=950.ckpt' as top 1\nINFO: Epoch 19, global step 1000: 'train_loss' reached 0.48954 (best 0.48954), saving model to '/kaggle/working/lightning_logs/version_19/checkpoints/epoch=19-step=1000.ckpt' as top 1\nINFO: `Trainer.fit` stopped: `max_epochs=20` reached.\n[I 2024-05-06 14:29:28,803] Trial 7 finished with value: 248.18247456668416 and parameters: {'num_layers': 3, 'hidden_size': 25, 'lr': 0.001, 'batch_size': 64}. Best is trial 6 with value: 189.0260339682677.\nINFO: GPU available: True (cuda), used: True\nINFO: TPU available: False, using: 0 TPU cores\nINFO: IPU available: False, using: 0 IPUs\nINFO: HPU available: False, using: 0 HPUs\n/opt/conda/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:74: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\nINFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\nINFO: \n  | Name  | Type        | Params | In sizes                                                         | Out sizes    \n-------------------------------------------------------------------------------------------------------------------------\n0 | model | DeepARModel | 20.5 K | [[1, 1], [1, 1], [1, 1242, 4], [1, 1242], [1, 1242], [1, 15, 4]] | [1, 1000, 15]\n-------------------------------------------------------------------------------------------------------------------------\n20.5 K    Trainable params\n0         Non-trainable params\n20.5 K    Total params\n0.082     Total estimated model params size (MB)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"04921e22bf044f77962d4a2ea4beb3ef"}},"metadata":{}},{"name":"stderr","text":"INFO: Epoch 0, global step 50: 'train_loss' reached 1.05573 (best 1.05573), saving model to '/kaggle/working/lightning_logs/version_20/checkpoints/epoch=0-step=50.ckpt' as top 1\nINFO: Epoch 1, global step 100: 'train_loss' reached 0.89814 (best 0.89814), saving model to '/kaggle/working/lightning_logs/version_20/checkpoints/epoch=1-step=100.ckpt' as top 1\nINFO: Epoch 2, global step 150: 'train_loss' reached 0.84869 (best 0.84869), saving model to '/kaggle/working/lightning_logs/version_20/checkpoints/epoch=2-step=150.ckpt' as top 1\nINFO: Epoch 3, global step 200: 'train_loss' reached 0.82166 (best 0.82166), saving model to '/kaggle/working/lightning_logs/version_20/checkpoints/epoch=3-step=200.ckpt' as top 1\nINFO: Epoch 4, global step 250: 'train_loss' reached 0.78345 (best 0.78345), saving model to '/kaggle/working/lightning_logs/version_20/checkpoints/epoch=4-step=250.ckpt' as top 1\nINFO: Epoch 5, global step 300: 'train_loss' reached 0.73417 (best 0.73417), saving model to '/kaggle/working/lightning_logs/version_20/checkpoints/epoch=5-step=300.ckpt' as top 1\nINFO: Epoch 6, global step 350: 'train_loss' reached 0.72945 (best 0.72945), saving model to '/kaggle/working/lightning_logs/version_20/checkpoints/epoch=6-step=350.ckpt' as top 1\nINFO: Epoch 7, global step 400: 'train_loss' reached 0.70286 (best 0.70286), saving model to '/kaggle/working/lightning_logs/version_20/checkpoints/epoch=7-step=400.ckpt' as top 1\nINFO: Epoch 8, global step 450: 'train_loss' reached 0.68985 (best 0.68985), saving model to '/kaggle/working/lightning_logs/version_20/checkpoints/epoch=8-step=450.ckpt' as top 1\nINFO: Epoch 9, global step 500: 'train_loss' reached 0.66826 (best 0.66826), saving model to '/kaggle/working/lightning_logs/version_20/checkpoints/epoch=9-step=500.ckpt' as top 1\nINFO: Epoch 10, global step 550: 'train_loss' was not in top 1\nINFO: Epoch 11, global step 600: 'train_loss' reached 0.64164 (best 0.64164), saving model to '/kaggle/working/lightning_logs/version_20/checkpoints/epoch=11-step=600.ckpt' as top 1\nINFO: Epoch 12, global step 650: 'train_loss' reached 0.61060 (best 0.61060), saving model to '/kaggle/working/lightning_logs/version_20/checkpoints/epoch=12-step=650.ckpt' as top 1\nINFO: Epoch 13, global step 700: 'train_loss' was not in top 1\nINFO: Epoch 14, global step 750: 'train_loss' reached 0.59189 (best 0.59189), saving model to '/kaggle/working/lightning_logs/version_20/checkpoints/epoch=14-step=750.ckpt' as top 1\nINFO: Epoch 15, global step 800: 'train_loss' was not in top 1\nINFO: Epoch 16, global step 850: 'train_loss' reached 0.54182 (best 0.54182), saving model to '/kaggle/working/lightning_logs/version_20/checkpoints/epoch=16-step=850.ckpt' as top 1\nINFO: Epoch 17, global step 900: 'train_loss' reached 0.51497 (best 0.51497), saving model to '/kaggle/working/lightning_logs/version_20/checkpoints/epoch=17-step=900.ckpt' as top 1\nINFO: Epoch 18, global step 950: 'train_loss' reached 0.49946 (best 0.49946), saving model to '/kaggle/working/lightning_logs/version_20/checkpoints/epoch=18-step=950.ckpt' as top 1\nINFO: Epoch 19, global step 1000: 'train_loss' was not in top 1\nINFO: `Trainer.fit` stopped: `max_epochs=20` reached.\n[I 2024-05-06 14:29:55,131] Trial 8 finished with value: 180.60188403658893 and parameters: {'num_layers': 2, 'hidden_size': 35, 'lr': 0.001, 'batch_size': 64}. Best is trial 8 with value: 180.60188403658893.\nINFO: GPU available: True (cuda), used: True\nINFO: TPU available: False, using: 0 TPU cores\nINFO: IPU available: False, using: 0 IPUs\nINFO: HPU available: False, using: 0 HPUs\n/opt/conda/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:74: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\nINFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\nINFO: \n  | Name  | Type        | Params | In sizes                                                         | Out sizes    \n-------------------------------------------------------------------------------------------------------------------------\n0 | model | DeepARModel | 33.8 K | [[1, 1], [1, 1], [1, 1242, 4], [1, 1242], [1, 1242], [1, 15, 4]] | [1, 1000, 15]\n-------------------------------------------------------------------------------------------------------------------------\n33.8 K    Trainable params\n0         Non-trainable params\n33.8 K    Total params\n0.135     Total estimated model params size (MB)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a88d9b40b44342edb76f11e03cad5187"}},"metadata":{}},{"name":"stderr","text":"INFO: Epoch 0, global step 50: 'train_loss' reached 1.21555 (best 1.21555), saving model to '/kaggle/working/lightning_logs/version_21/checkpoints/epoch=0-step=50.ckpt' as top 1\nINFO: Epoch 1, global step 100: 'train_loss' reached 1.13782 (best 1.13782), saving model to '/kaggle/working/lightning_logs/version_21/checkpoints/epoch=1-step=100.ckpt' as top 1\nINFO: Epoch 2, global step 150: 'train_loss' reached 1.07067 (best 1.07067), saving model to '/kaggle/working/lightning_logs/version_21/checkpoints/epoch=2-step=150.ckpt' as top 1\nINFO: Epoch 3, global step 200: 'train_loss' reached 1.04431 (best 1.04431), saving model to '/kaggle/working/lightning_logs/version_21/checkpoints/epoch=3-step=200.ckpt' as top 1\nINFO: Epoch 4, global step 250: 'train_loss' reached 0.98134 (best 0.98134), saving model to '/kaggle/working/lightning_logs/version_21/checkpoints/epoch=4-step=250.ckpt' as top 1\nINFO: Epoch 5, global step 300: 'train_loss' was not in top 1\nINFO: Epoch 6, global step 350: 'train_loss' reached 0.97145 (best 0.97145), saving model to '/kaggle/working/lightning_logs/version_21/checkpoints/epoch=6-step=350.ckpt' as top 1\nINFO: Epoch 7, global step 400: 'train_loss' reached 0.95594 (best 0.95594), saving model to '/kaggle/working/lightning_logs/version_21/checkpoints/epoch=7-step=400.ckpt' as top 1\nINFO: Epoch 8, global step 450: 'train_loss' reached 0.93994 (best 0.93994), saving model to '/kaggle/working/lightning_logs/version_21/checkpoints/epoch=8-step=450.ckpt' as top 1\nINFO: Epoch 9, global step 500: 'train_loss' reached 0.93107 (best 0.93107), saving model to '/kaggle/working/lightning_logs/version_21/checkpoints/epoch=9-step=500.ckpt' as top 1\nINFO: Epoch 10, global step 550: 'train_loss' reached 0.91438 (best 0.91438), saving model to '/kaggle/working/lightning_logs/version_21/checkpoints/epoch=10-step=550.ckpt' as top 1\nINFO: Epoch 11, global step 600: 'train_loss' was not in top 1\nINFO: Epoch 12, global step 650: 'train_loss' reached 0.91255 (best 0.91255), saving model to '/kaggle/working/lightning_logs/version_21/checkpoints/epoch=12-step=650.ckpt' as top 1\nINFO: Epoch 13, global step 700: 'train_loss' reached 0.89228 (best 0.89228), saving model to '/kaggle/working/lightning_logs/version_21/checkpoints/epoch=13-step=700.ckpt' as top 1\nINFO: Epoch 14, global step 750: 'train_loss' was not in top 1\nINFO: Epoch 15, global step 800: 'train_loss' reached 0.89096 (best 0.89096), saving model to '/kaggle/working/lightning_logs/version_21/checkpoints/epoch=15-step=800.ckpt' as top 1\nINFO: Epoch 16, global step 850: 'train_loss' reached 0.88166 (best 0.88166), saving model to '/kaggle/working/lightning_logs/version_21/checkpoints/epoch=16-step=850.ckpt' as top 1\nINFO: Epoch 17, global step 900: 'train_loss' was not in top 1\nINFO: Epoch 18, global step 950: 'train_loss' was not in top 1\nINFO: Epoch 19, global step 1000: 'train_loss' reached 0.87145 (best 0.87145), saving model to '/kaggle/working/lightning_logs/version_21/checkpoints/epoch=19-step=1000.ckpt' as top 1\nINFO: `Trainer.fit` stopped: `max_epochs=20` reached.\n[I 2024-05-06 14:30:18,324] Trial 9 finished with value: 229.64510139774816 and parameters: {'num_layers': 3, 'hidden_size': 37, 'lr': 0.0001, 'batch_size': 48}. Best is trial 8 with value: 180.60188403658893.\n","output_type":"stream"},{"name":"stdout","text":"Number of finished trials: 10\nBest trial:\n  Value: 180.60188403658893\n  Params: \n    num_layers: 2\n    hidden_size: 35\n    lr: 0.001\n    batch_size: 64\n240.51415395736694\n","output_type":"stream"}]}]}