{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":8282425,"sourceType":"datasetVersion","datasetId":4918955}],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install 'gluonts[torch]'","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-05-02T08:07:17.891025Z","iopub.execute_input":"2024-05-02T08:07:17.891400Z","iopub.status.idle":"2024-05-02T08:07:37.970807Z","shell.execute_reply.started":"2024-05-02T08:07:17.891370Z","shell.execute_reply":"2024-05-02T08:07:37.969762Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting gluonts[torch]\n  Downloading gluonts-0.14.4-py3-none-any.whl.metadata (9.5 kB)\nRequirement already satisfied: numpy~=1.16 in /opt/conda/lib/python3.10/site-packages (from gluonts[torch]) (1.26.4)\nRequirement already satisfied: pandas<2.2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from gluonts[torch]) (2.1.4)\nRequirement already satisfied: pydantic<3,>=1.7 in /opt/conda/lib/python3.10/site-packages (from gluonts[torch]) (2.5.3)\nRequirement already satisfied: tqdm~=4.23 in /opt/conda/lib/python3.10/site-packages (from gluonts[torch]) (4.66.1)\nRequirement already satisfied: toolz~=0.10 in /opt/conda/lib/python3.10/site-packages (from gluonts[torch]) (0.12.1)\nRequirement already satisfied: typing-extensions~=4.0 in /opt/conda/lib/python3.10/site-packages (from gluonts[torch]) (4.9.0)\nRequirement already satisfied: torch<3,>=1.9 in /opt/conda/lib/python3.10/site-packages (from gluonts[torch]) (2.1.2)\nCollecting lightning<2.2,>=2.0 (from gluonts[torch])\n  Downloading lightning-2.1.4-py3-none-any.whl.metadata (57 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.2/57.2 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting pytorch-lightning<2.2,>=2.0 (from gluonts[torch])\n  Downloading pytorch_lightning-2.1.4-py3-none-any.whl.metadata (21 kB)\nRequirement already satisfied: scipy~=1.10 in /opt/conda/lib/python3.10/site-packages (from gluonts[torch]) (1.11.4)\nRequirement already satisfied: PyYAML<8.0,>=5.4 in /opt/conda/lib/python3.10/site-packages (from lightning<2.2,>=2.0->gluonts[torch]) (6.0.1)\nRequirement already satisfied: fsspec<2025.0,>=2022.5.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<2025.0,>=2022.5.0->lightning<2.2,>=2.0->gluonts[torch]) (2024.2.0)\nRequirement already satisfied: lightning-utilities<2.0,>=0.8.0 in /opt/conda/lib/python3.10/site-packages (from lightning<2.2,>=2.0->gluonts[torch]) (0.11.2)\nRequirement already satisfied: packaging<25.0,>=20.0 in /opt/conda/lib/python3.10/site-packages (from lightning<2.2,>=2.0->gluonts[torch]) (21.3)\nRequirement already satisfied: torchmetrics<3.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from lightning<2.2,>=2.0->gluonts[torch]) (1.3.2)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas<2.2.0,>=1.0->gluonts[torch]) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas<2.2.0,>=1.0->gluonts[torch]) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas<2.2.0,>=1.0->gluonts[torch]) (2023.4)\nRequirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from pydantic<3,>=1.7->gluonts[torch]) (0.6.0)\nRequirement already satisfied: pydantic-core==2.14.6 in /opt/conda/lib/python3.10/site-packages (from pydantic<3,>=1.7->gluonts[torch]) (2.14.6)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch<3,>=1.9->gluonts[torch]) (3.13.1)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch<3,>=1.9->gluonts[torch]) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch<3,>=1.9->gluonts[torch]) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch<3,>=1.9->gluonts[torch]) (3.1.2)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<2025.0,>=2022.5.0->lightning<2.2,>=2.0->gluonts[torch]) (3.9.1)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from lightning-utilities<2.0,>=0.8.0->lightning<2.2,>=2.0->gluonts[torch]) (69.0.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging<25.0,>=20.0->lightning<2.2,>=2.0->gluonts[torch]) (3.1.1)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas<2.2.0,>=1.0->gluonts[torch]) (1.16.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch<3,>=1.9->gluonts[torch]) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch<3,>=1.9->gluonts[torch]) (1.3.0)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning<2.2,>=2.0->gluonts[torch]) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning<2.2,>=2.0->gluonts[torch]) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning<2.2,>=2.0->gluonts[torch]) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning<2.2,>=2.0->gluonts[torch]) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning<2.2,>=2.0->gluonts[torch]) (1.3.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning<2.2,>=2.0->gluonts[torch]) (4.0.3)\nRequirement already satisfied: idna>=2.0 in /opt/conda/lib/python3.10/site-packages (from yarl<2.0,>=1.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning<2.2,>=2.0->gluonts[torch]) (3.6)\nDownloading lightning-2.1.4-py3-none-any.whl (2.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m28.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading pytorch_lightning-2.1.4-py3-none-any.whl (778 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m778.1/778.1 kB\u001b[0m \u001b[31m39.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading gluonts-0.14.4-py3-none-any.whl (1.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m56.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: gluonts, pytorch-lightning, lightning\n  Attempting uninstall: pytorch-lightning\n    Found existing installation: pytorch-lightning 2.2.2\n    Uninstalling pytorch-lightning-2.2.2:\n      Successfully uninstalled pytorch-lightning-2.2.2\nSuccessfully installed gluonts-0.14.4 lightning-2.1.4 pytorch-lightning-2.1.4\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install optuna","metadata":{"execution":{"iopub.status.busy":"2024-05-02T08:07:49.328424Z","iopub.execute_input":"2024-05-02T08:07:49.328809Z","iopub.status.idle":"2024-05-02T08:08:02.973926Z","shell.execute_reply.started":"2024-05-02T08:07:49.328775Z","shell.execute_reply":"2024-05-02T08:08:02.973027Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Requirement already satisfied: optuna in /opt/conda/lib/python3.10/site-packages (3.6.1)\nRequirement already satisfied: alembic>=1.5.0 in /opt/conda/lib/python3.10/site-packages (from optuna) (1.13.1)\nRequirement already satisfied: colorlog in /opt/conda/lib/python3.10/site-packages (from optuna) (6.8.2)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from optuna) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from optuna) (21.3)\nRequirement already satisfied: sqlalchemy>=1.3.0 in /opt/conda/lib/python3.10/site-packages (from optuna) (2.0.25)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from optuna) (4.66.1)\nRequirement already satisfied: PyYAML in /opt/conda/lib/python3.10/site-packages (from optuna) (6.0.1)\nRequirement already satisfied: Mako in /opt/conda/lib/python3.10/site-packages (from alembic>=1.5.0->optuna) (1.3.3)\nRequirement already satisfied: typing-extensions>=4 in /opt/conda/lib/python3.10/site-packages (from alembic>=1.5.0->optuna) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->optuna) (3.1.1)\nRequirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.10/site-packages (from sqlalchemy>=1.3.0->optuna) (3.0.3)\nRequirement already satisfied: MarkupSafe>=0.9.2 in /opt/conda/lib/python3.10/site-packages (from Mako->alembic>=1.5.0->optuna) (2.1.3)\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom gluonts.dataset.pandas import PandasDataset\nfrom gluonts.torch.model.deepar import DeepAREstimator\nfrom gluonts.torch.distributions import NormalOutput\nfrom gluonts.evaluation import make_evaluation_predictions\nimport matplotlib.pyplot as plt\nimport json\n\n\npath = '../input/tourism-data/'\n# Split train and test\ndf = pd.read_csv(path+'Tourism_process_for_deepar.csv')\ndf.set_index('Date',inplace=True)\n#df.index = pd.to_datetime(df.index).dt.strftime('%Y-%m-%d %H:%M:%S')\nprediction_length = 12\nfreq = 'MS'\nsplit_date = pd.to_datetime('2016-01-01').strftime('%Y-%m-%d %H:%M:%S')\ntrain = df[df.index < split_date]\ntest = df[df.index >= split_date]","metadata":{"execution":{"iopub.status.busy":"2024-05-02T08:08:05.708969Z","iopub.execute_input":"2024-05-02T08:08:05.709364Z","iopub.status.idle":"2024-05-02T08:08:14.687647Z","shell.execute_reply.started":"2024-05-02T08:08:05.709328Z","shell.execute_reply":"2024-05-02T08:08:14.686844Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"split_date1 = pd.to_datetime('2015-01-01').strftime('%Y-%m-%d %H:%M:%S')\ntrain1 = df[df.index < split_date1]\nval = train[train.index>=split_date1]","metadata":{"execution":{"iopub.status.busy":"2024-05-02T08:08:18.313848Z","iopub.execute_input":"2024-05-02T08:08:18.314645Z","iopub.status.idle":"2024-05-02T08:08:18.336255Z","shell.execute_reply.started":"2024-05-02T08:08:18.314599Z","shell.execute_reply":"2024-05-02T08:08:18.335072Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"train1","metadata":{"execution":{"iopub.status.busy":"2024-05-02T08:08:20.343543Z","iopub.execute_input":"2024-05-02T08:08:20.343942Z","iopub.status.idle":"2024-05-02T08:08:20.365447Z","shell.execute_reply.started":"2024-05-02T08:08:20.343910Z","shell.execute_reply":"2024-05-02T08:08:20.364281Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"                    Node         Value  Month_Of_Year  Region  Zone  State\nDate                                                                      \n1998-01-01 00:00:00    T  45151.071280              1       0     0      0\n1998-01-01 00:00:00    A  17515.502380              1       0     0      1\n1998-01-01 00:00:00    B  10393.618016              1       0     0      2\n1998-01-01 00:00:00    C   8633.359047              1       0     0      3\n1998-01-01 00:00:00    D   3504.313346              1       0     0      4\n...                  ...           ...            ...     ...   ...    ...\n2014-12-01 00:00:00  GAC     29.792837             12      72    26      7\n2014-12-01 00:00:00  GBA     30.358611             12      73    27      7\n2014-12-01 00:00:00  GBB      1.530844             12      74    27      7\n2014-12-01 00:00:00  GBC     68.905708             12      75    27      7\n2014-12-01 00:00:00  GBD      5.211008             12      76    27      7\n\n[22644 rows x 6 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Node</th>\n      <th>Value</th>\n      <th>Month_Of_Year</th>\n      <th>Region</th>\n      <th>Zone</th>\n      <th>State</th>\n    </tr>\n    <tr>\n      <th>Date</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1998-01-01 00:00:00</th>\n      <td>T</td>\n      <td>45151.071280</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1998-01-01 00:00:00</th>\n      <td>A</td>\n      <td>17515.502380</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1998-01-01 00:00:00</th>\n      <td>B</td>\n      <td>10393.618016</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>1998-01-01 00:00:00</th>\n      <td>C</td>\n      <td>8633.359047</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>1998-01-01 00:00:00</th>\n      <td>D</td>\n      <td>3504.313346</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2014-12-01 00:00:00</th>\n      <td>GAC</td>\n      <td>29.792837</td>\n      <td>12</td>\n      <td>72</td>\n      <td>26</td>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>2014-12-01 00:00:00</th>\n      <td>GBA</td>\n      <td>30.358611</td>\n      <td>12</td>\n      <td>73</td>\n      <td>27</td>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>2014-12-01 00:00:00</th>\n      <td>GBB</td>\n      <td>1.530844</td>\n      <td>12</td>\n      <td>74</td>\n      <td>27</td>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>2014-12-01 00:00:00</th>\n      <td>GBC</td>\n      <td>68.905708</td>\n      <td>12</td>\n      <td>75</td>\n      <td>27</td>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>2014-12-01 00:00:00</th>\n      <td>GBD</td>\n      <td>5.211008</td>\n      <td>12</td>\n      <td>76</td>\n      <td>27</td>\n      <td>7</td>\n    </tr>\n  </tbody>\n</table>\n<p>22644 rows × 6 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"train1.reset_index(inplace=True)\nval.reset_index(inplace=True)\n\ntrain2 = train1.drop_duplicates(subset=['Node','Region','Zone','State'])\ntrain_static = pd.DataFrame({'State':train2['State'],\n                             'Zone':train2['Zone'],\n                             'Region':train2['Region'],\n                             'Node':train2['Node']})\ntrain_static.set_index('Node',inplace=True)\n\ntrain_group = train1.groupby('Node')\nstandardized_params = {}\ntrain_standard = train1.copy()\n\n# 标准化\nfor cat,group in train_group:\n    means = group['Value'].mean()\n    stds = group['Value'].std()\n    standardized_params[cat] = {'mean':means,'std':stds}\n    train_standard.loc[group.index,'Value'] = (group['Value']-means)/stds\n","metadata":{"execution":{"iopub.status.busy":"2024-05-02T08:08:24.524664Z","iopub.execute_input":"2024-05-02T08:08:24.525113Z","iopub.status.idle":"2024-05-02T08:08:24.675735Z","shell.execute_reply.started":"2024-05-02T08:08:24.525082Z","shell.execute_reply":"2024-05-02T08:08:24.674740Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"train_standard","metadata":{"execution":{"iopub.status.busy":"2024-05-02T08:08:27.771940Z","iopub.execute_input":"2024-05-02T08:08:27.772343Z","iopub.status.idle":"2024-05-02T08:08:27.787329Z","shell.execute_reply.started":"2024-05-02T08:08:27.772311Z","shell.execute_reply":"2024-05-02T08:08:27.786312Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"                      Date Node     Value  Month_Of_Year  Region  Zone  State\n0      1998-01-01 00:00:00    T  3.274775              1       0     0      0\n1      1998-01-01 00:00:00    A  3.854917              1       0     0      1\n2      1998-01-01 00:00:00    B  3.319479              1       0     0      2\n3      1998-01-01 00:00:00    C  1.583920              1       0     0      3\n4      1998-01-01 00:00:00    D  3.559008              1       0     0      4\n...                    ...  ...       ...            ...     ...   ...    ...\n22639  2014-12-01 00:00:00  GAC -0.589784             12      72    26      7\n22640  2014-12-01 00:00:00  GBA  0.742826             12      73    27      7\n22641  2014-12-01 00:00:00  GBB -1.003333             12      74    27      7\n22642  2014-12-01 00:00:00  GBC -0.224970             12      75    27      7\n22643  2014-12-01 00:00:00  GBD -0.502026             12      76    27      7\n\n[22644 rows x 7 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Date</th>\n      <th>Node</th>\n      <th>Value</th>\n      <th>Month_Of_Year</th>\n      <th>Region</th>\n      <th>Zone</th>\n      <th>State</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1998-01-01 00:00:00</td>\n      <td>T</td>\n      <td>3.274775</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1998-01-01 00:00:00</td>\n      <td>A</td>\n      <td>3.854917</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1998-01-01 00:00:00</td>\n      <td>B</td>\n      <td>3.319479</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1998-01-01 00:00:00</td>\n      <td>C</td>\n      <td>1.583920</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1998-01-01 00:00:00</td>\n      <td>D</td>\n      <td>3.559008</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>22639</th>\n      <td>2014-12-01 00:00:00</td>\n      <td>GAC</td>\n      <td>-0.589784</td>\n      <td>12</td>\n      <td>72</td>\n      <td>26</td>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>22640</th>\n      <td>2014-12-01 00:00:00</td>\n      <td>GBA</td>\n      <td>0.742826</td>\n      <td>12</td>\n      <td>73</td>\n      <td>27</td>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>22641</th>\n      <td>2014-12-01 00:00:00</td>\n      <td>GBB</td>\n      <td>-1.003333</td>\n      <td>12</td>\n      <td>74</td>\n      <td>27</td>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>22642</th>\n      <td>2014-12-01 00:00:00</td>\n      <td>GBC</td>\n      <td>-0.224970</td>\n      <td>12</td>\n      <td>75</td>\n      <td>27</td>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>22643</th>\n      <td>2014-12-01 00:00:00</td>\n      <td>GBD</td>\n      <td>-0.502026</td>\n      <td>12</td>\n      <td>76</td>\n      <td>27</td>\n      <td>7</td>\n    </tr>\n  </tbody>\n</table>\n<p>22644 rows × 7 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"train_ds = PandasDataset.from_long_dataframe(train_standard.iloc[:,[0,1,2,3]],\n                                             target=\"Value\",\n                                             timestamp='Date',\n                                             freq='M',\n                                             item_id=\"Node\",\n                                             feat_dynamic_real=[\"Month_Of_Year\"],\n                                             static_features=train_static)","metadata":{"execution":{"iopub.status.busy":"2024-05-02T08:08:31.919876Z","iopub.execute_input":"2024-05-02T08:08:31.920583Z","iopub.status.idle":"2024-05-02T08:08:31.949566Z","shell.execute_reply.started":"2024-05-02T08:08:31.920549Z","shell.execute_reply":"2024-05-02T08:08:31.948614Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"val","metadata":{"execution":{"iopub.status.busy":"2024-05-02T08:08:35.408590Z","iopub.execute_input":"2024-05-02T08:08:35.409289Z","iopub.status.idle":"2024-05-02T08:08:35.423307Z","shell.execute_reply.started":"2024-05-02T08:08:35.409250Z","shell.execute_reply":"2024-05-02T08:08:35.422241Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"                     Date Node         Value  Month_Of_Year  Region  Zone  \\\n0     2015-01-01 00:00:00    T  44072.739245              1       0     0   \n1     2015-01-01 00:00:00    A  14467.899039              1       0     0   \n2     2015-01-01 00:00:00    B   9989.135051              1       0     0   \n3     2015-01-01 00:00:00    C   8928.542621              1       0     0   \n4     2015-01-01 00:00:00    D   3705.183969              1       0     0   \n...                   ...  ...           ...            ...     ...   ...   \n1327  2015-12-01 00:00:00  GAC     20.828564             12      72    26   \n1328  2015-12-01 00:00:00  GBA     11.350454             12      73    27   \n1329  2015-12-01 00:00:00  GBB      0.000000             12      74    27   \n1330  2015-12-01 00:00:00  GBC    109.454456             12      75    27   \n1331  2015-12-01 00:00:00  GBD     31.486646             12      76    27   \n\n      State  \n0         0  \n1         1  \n2         2  \n3         3  \n4         4  \n...     ...  \n1327      7  \n1328      7  \n1329      7  \n1330      7  \n1331      7  \n\n[1332 rows x 7 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Date</th>\n      <th>Node</th>\n      <th>Value</th>\n      <th>Month_Of_Year</th>\n      <th>Region</th>\n      <th>Zone</th>\n      <th>State</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2015-01-01 00:00:00</td>\n      <td>T</td>\n      <td>44072.739245</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2015-01-01 00:00:00</td>\n      <td>A</td>\n      <td>14467.899039</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2015-01-01 00:00:00</td>\n      <td>B</td>\n      <td>9989.135051</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2015-01-01 00:00:00</td>\n      <td>C</td>\n      <td>8928.542621</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2015-01-01 00:00:00</td>\n      <td>D</td>\n      <td>3705.183969</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1327</th>\n      <td>2015-12-01 00:00:00</td>\n      <td>GAC</td>\n      <td>20.828564</td>\n      <td>12</td>\n      <td>72</td>\n      <td>26</td>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>1328</th>\n      <td>2015-12-01 00:00:00</td>\n      <td>GBA</td>\n      <td>11.350454</td>\n      <td>12</td>\n      <td>73</td>\n      <td>27</td>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>1329</th>\n      <td>2015-12-01 00:00:00</td>\n      <td>GBB</td>\n      <td>0.000000</td>\n      <td>12</td>\n      <td>74</td>\n      <td>27</td>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>1330</th>\n      <td>2015-12-01 00:00:00</td>\n      <td>GBC</td>\n      <td>109.454456</td>\n      <td>12</td>\n      <td>75</td>\n      <td>27</td>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>1331</th>\n      <td>2015-12-01 00:00:00</td>\n      <td>GBD</td>\n      <td>31.486646</td>\n      <td>12</td>\n      <td>76</td>\n      <td>27</td>\n      <td>7</td>\n    </tr>\n  </tbody>\n</table>\n<p>1332 rows × 7 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"val_group = val.groupby('Node')\nval_standard = val.copy()\n\n# 标准化\nfor cat,group in val_group:\n    means = group['Value'].mean()\n    stds = group['Value'].std()\n    val_standard.loc[group.index,'Value'] = (group['Value']-means)/stds","metadata":{"execution":{"iopub.status.busy":"2024-05-02T08:08:41.230986Z","iopub.execute_input":"2024-05-02T08:08:41.231633Z","iopub.status.idle":"2024-05-02T08:08:41.373351Z","shell.execute_reply.started":"2024-05-02T08:08:41.231604Z","shell.execute_reply":"2024-05-02T08:08:41.372278Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"val_standard","metadata":{"execution":{"iopub.status.busy":"2024-05-02T08:08:43.647698Z","iopub.execute_input":"2024-05-02T08:08:43.648388Z","iopub.status.idle":"2024-05-02T08:08:43.661730Z","shell.execute_reply.started":"2024-05-02T08:08:43.648354Z","shell.execute_reply":"2024-05-02T08:08:43.660629Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"                     Date Node     Value  Month_Of_Year  Region  Zone  State\n0     2015-01-01 00:00:00    T  2.950906              1       0     0      0\n1     2015-01-01 00:00:00    A  2.924239              1       0     0      1\n2     2015-01-01 00:00:00    B  2.978696              1       0     0      2\n3     2015-01-01 00:00:00    C  1.889177              1       0     0      3\n4     2015-01-01 00:00:00    D  2.854171              1       0     0      4\n...                   ...  ...       ...            ...     ...   ...    ...\n1327  2015-12-01 00:00:00  GAC -0.763644             12      72    26      7\n1328  2015-12-01 00:00:00  GBA -0.168195             12      73    27      7\n1329  2015-12-01 00:00:00  GBB -1.302586             12      74    27      7\n1330  2015-12-01 00:00:00  GBC  0.297599             12      75    27      7\n1331  2015-12-01 00:00:00  GBD  0.463693             12      76    27      7\n\n[1332 rows x 7 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Date</th>\n      <th>Node</th>\n      <th>Value</th>\n      <th>Month_Of_Year</th>\n      <th>Region</th>\n      <th>Zone</th>\n      <th>State</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2015-01-01 00:00:00</td>\n      <td>T</td>\n      <td>2.950906</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2015-01-01 00:00:00</td>\n      <td>A</td>\n      <td>2.924239</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2015-01-01 00:00:00</td>\n      <td>B</td>\n      <td>2.978696</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2015-01-01 00:00:00</td>\n      <td>C</td>\n      <td>1.889177</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2015-01-01 00:00:00</td>\n      <td>D</td>\n      <td>2.854171</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1327</th>\n      <td>2015-12-01 00:00:00</td>\n      <td>GAC</td>\n      <td>-0.763644</td>\n      <td>12</td>\n      <td>72</td>\n      <td>26</td>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>1328</th>\n      <td>2015-12-01 00:00:00</td>\n      <td>GBA</td>\n      <td>-0.168195</td>\n      <td>12</td>\n      <td>73</td>\n      <td>27</td>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>1329</th>\n      <td>2015-12-01 00:00:00</td>\n      <td>GBB</td>\n      <td>-1.302586</td>\n      <td>12</td>\n      <td>74</td>\n      <td>27</td>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>1330</th>\n      <td>2015-12-01 00:00:00</td>\n      <td>GBC</td>\n      <td>0.297599</td>\n      <td>12</td>\n      <td>75</td>\n      <td>27</td>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>1331</th>\n      <td>2015-12-01 00:00:00</td>\n      <td>GBD</td>\n      <td>0.463693</td>\n      <td>12</td>\n      <td>76</td>\n      <td>27</td>\n      <td>7</td>\n    </tr>\n  </tbody>\n</table>\n<p>1332 rows × 7 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"estimator = DeepAREstimator(\n            num_layers=3,\n            hidden_size=40,\n            lr=1e-3,\n            prediction_length=12,\n            context_length=10*prediction_length,\n            patience = 10,\n            freq='MS',\n            distr_output=NormalOutput(),\n            scaling=False,\n            num_parallel_samples=1000,\n            batch_size=32,\n            trainer_kwargs={\n                \"max_epochs\": 20,\n            },\n        )\n\npredictor = estimator.train(train_ds)\nforecast_it = predictor.predict(train_ds)\n\nforecasts = list(forecast_it)","metadata":{"execution":{"iopub.status.busy":"2024-05-02T08:16:30.704563Z","iopub.execute_input":"2024-05-02T08:16:30.705237Z","iopub.status.idle":"2024-05-02T08:17:43.541356Z","shell.execute_reply.started":"2024-05-02T08:16:30.705202Z","shell.execute_reply":"2024-05-02T08:17:43.540511Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stderr","text":"INFO: GPU available: True (cuda), used: True\nINFO: TPU available: False, using: 0 TPU cores\nINFO: IPU available: False, using: 0 IPUs\nINFO: HPU available: False, using: 0 HPUs\n/opt/conda/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:74: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\nWARNING: Missing logger folder: /kaggle/working/lightning_logs\n2024-05-02 08:16:34.330860: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-05-02 08:16:34.330990: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-05-02 08:16:34.461893: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nINFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\nINFO: \n  | Name  | Type        | Params | In sizes                                                      | Out sizes    \n----------------------------------------------------------------------------------------------------------------------\n0 | model | DeepARModel | 36.4 K | [[1, 1], [1, 1], [1, 156, 2], [1, 156], [1, 156], [1, 12, 2]] | [1, 1000, 12]\n----------------------------------------------------------------------------------------------------------------------\n36.4 K    Trainable params\n0         Non-trainable params\n36.4 K    Total params\n0.146     Total estimated model params size (MB)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"03b4d32a42814466b13dea254ce85ed8"}},"metadata":{}},{"name":"stderr","text":"INFO: Epoch 0, global step 50: 'train_loss' reached 1.01100 (best 1.01100), saving model to '/kaggle/working/lightning_logs/version_0/checkpoints/epoch=0-step=50.ckpt' as top 1\nINFO: Epoch 1, global step 100: 'train_loss' reached 0.89567 (best 0.89567), saving model to '/kaggle/working/lightning_logs/version_0/checkpoints/epoch=1-step=100.ckpt' as top 1\nINFO: Epoch 2, global step 150: 'train_loss' reached 0.83817 (best 0.83817), saving model to '/kaggle/working/lightning_logs/version_0/checkpoints/epoch=2-step=150.ckpt' as top 1\nINFO: Epoch 3, global step 200: 'train_loss' reached 0.79543 (best 0.79543), saving model to '/kaggle/working/lightning_logs/version_0/checkpoints/epoch=3-step=200.ckpt' as top 1\nINFO: Epoch 4, global step 250: 'train_loss' reached 0.76394 (best 0.76394), saving model to '/kaggle/working/lightning_logs/version_0/checkpoints/epoch=4-step=250.ckpt' as top 1\nINFO: Epoch 5, global step 300: 'train_loss' reached 0.74717 (best 0.74717), saving model to '/kaggle/working/lightning_logs/version_0/checkpoints/epoch=5-step=300.ckpt' as top 1\nINFO: Epoch 6, global step 350: 'train_loss' was not in top 1\nINFO: Epoch 7, global step 400: 'train_loss' reached 0.74183 (best 0.74183), saving model to '/kaggle/working/lightning_logs/version_0/checkpoints/epoch=7-step=400.ckpt' as top 1\nINFO: Epoch 8, global step 450: 'train_loss' was not in top 1\nINFO: Epoch 9, global step 500: 'train_loss' reached 0.73175 (best 0.73175), saving model to '/kaggle/working/lightning_logs/version_0/checkpoints/epoch=9-step=500.ckpt' as top 1\nINFO: Epoch 10, global step 550: 'train_loss' reached 0.71951 (best 0.71951), saving model to '/kaggle/working/lightning_logs/version_0/checkpoints/epoch=10-step=550.ckpt' as top 1\nINFO: Epoch 11, global step 600: 'train_loss' reached 0.69839 (best 0.69839), saving model to '/kaggle/working/lightning_logs/version_0/checkpoints/epoch=11-step=600.ckpt' as top 1\nINFO: Epoch 12, global step 650: 'train_loss' reached 0.69446 (best 0.69446), saving model to '/kaggle/working/lightning_logs/version_0/checkpoints/epoch=12-step=650.ckpt' as top 1\nINFO: Epoch 13, global step 700: 'train_loss' reached 0.68450 (best 0.68450), saving model to '/kaggle/working/lightning_logs/version_0/checkpoints/epoch=13-step=700.ckpt' as top 1\nINFO: Epoch 14, global step 750: 'train_loss' was not in top 1\nINFO: Epoch 15, global step 800: 'train_loss' reached 0.68314 (best 0.68314), saving model to '/kaggle/working/lightning_logs/version_0/checkpoints/epoch=15-step=800.ckpt' as top 1\nINFO: Epoch 16, global step 850: 'train_loss' reached 0.66005 (best 0.66005), saving model to '/kaggle/working/lightning_logs/version_0/checkpoints/epoch=16-step=850.ckpt' as top 1\nINFO: Epoch 17, global step 900: 'train_loss' was not in top 1\nINFO: Epoch 18, global step 950: 'train_loss' was not in top 1\nINFO: Epoch 19, global step 1000: 'train_loss' reached 0.64837 (best 0.64837), saving model to '/kaggle/working/lightning_logs/version_0/checkpoints/epoch=19-step=1000.ckpt' as top 1\nINFO: `Trainer.fit` stopped: `max_epochs=20` reached.\n","output_type":"stream"}]},{"cell_type":"code","source":"forecasts[0].samples","metadata":{"execution":{"iopub.status.busy":"2024-05-02T08:19:02.247146Z","iopub.execute_input":"2024-05-02T08:19:02.248018Z","iopub.status.idle":"2024-05-02T08:19:02.255019Z","shell.execute_reply.started":"2024-05-02T08:19:02.247981Z","shell.execute_reply":"2024-05-02T08:19:02.253974Z"},"trusted":true},"execution_count":21,"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"array([[ 2.757091  , -0.8080652 , -1.0843526 , ...,  0.42752808,\n        -0.6124935 , -0.72810507],\n       [ 3.0293152 , -0.6537782 , -0.5317347 , ...,  0.66828775,\n        -0.40513134, -0.16052985],\n       [ 2.580253  , -0.7947694 , -0.4406939 , ...,  0.3909461 ,\n        -0.44125494,  0.18742868],\n       ...,\n       [ 1.7074896 , -0.3810192 , -1.0399537 , ..., -0.04010795,\n         0.27660513, -0.64579606],\n       [ 2.2419195 , -0.5966748 , -0.00652245, ...,  0.49082142,\n        -0.6202163 , -0.63950914],\n       [ 2.4376235 , -0.6919148 , -0.42613304, ...,  0.73359096,\n        -0.38487917, -0.13849175]], dtype=float32)"},"metadata":{}}]},{"cell_type":"code","source":"node_nonsort = []\nfor i in range(len(forecasts)):\n    node_nonsort.append(forecasts[i].item_id)\nnew_df = pd.read_csv(path+'/Tourism_process.csv')\nnode_list = new_df.columns[2:].tolist()","metadata":{"execution":{"iopub.status.busy":"2024-05-02T08:24:27.506490Z","iopub.execute_input":"2024-05-02T08:24:27.506836Z","iopub.status.idle":"2024-05-02T08:24:27.525843Z","shell.execute_reply.started":"2024-05-02T08:24:27.506791Z","shell.execute_reply":"2024-05-02T08:24:27.524869Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"index_list = [node_nonsort.index(i) for i in node_list]","metadata":{"execution":{"iopub.status.busy":"2024-05-02T08:24:45.740467Z","iopub.execute_input":"2024-05-02T08:24:45.740994Z","iopub.status.idle":"2024-05-02T08:24:45.746218Z","shell.execute_reply.started":"2024-05-02T08:24:45.740954Z","shell.execute_reply":"2024-05-02T08:24:45.745111Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"forecasts[0].samples[:,0].shape","metadata":{"execution":{"iopub.status.busy":"2024-05-02T08:26:37.885858Z","iopub.execute_input":"2024-05-02T08:26:37.886611Z","iopub.status.idle":"2024-05-02T08:26:37.892534Z","shell.execute_reply.started":"2024-05-02T08:26:37.886580Z","shell.execute_reply":"2024-05-02T08:26:37.891671Z"},"trusted":true},"execution_count":33,"outputs":[{"execution_count":33,"output_type":"execute_result","data":{"text/plain":"(1000,)"},"metadata":{}}]},{"cell_type":"code","source":"samples_total = []\nfor j in range(12):\n    samples_step = []\n    for i in range(len(forecasts)):\n        samples_step.append(forecasts[i].samples[:,j].reshape((1,1000)))\n    samples_1 = []\n    for k in index_list:\n        samples_1.append(samples_step[k])\n    samples_total.append(np.concatenate(samples_1,axis=0))\n","metadata":{"execution":{"iopub.status.busy":"2024-05-02T08:45:14.895463Z","iopub.execute_input":"2024-05-02T08:45:14.896401Z","iopub.status.idle":"2024-05-02T08:45:14.910227Z","shell.execute_reply.started":"2024-05-02T08:45:14.896370Z","shell.execute_reply":"2024-05-02T08:45:14.909244Z"},"trusted":true},"execution_count":74,"outputs":[]},{"cell_type":"code","source":"samples_new = np.concatenate(samples_total,axis=1)","metadata":{"execution":{"iopub.status.busy":"2024-05-02T08:45:17.119600Z","iopub.execute_input":"2024-05-02T08:45:17.120377Z","iopub.status.idle":"2024-05-02T08:45:17.125773Z","shell.execute_reply.started":"2024-05-02T08:45:17.120342Z","shell.execute_reply":"2024-05-02T08:45:17.124842Z"},"trusted":true},"execution_count":75,"outputs":[]},{"cell_type":"code","source":"samples_new","metadata":{"execution":{"iopub.status.busy":"2024-05-02T09:10:04.796896Z","iopub.execute_input":"2024-05-02T09:10:04.797636Z","iopub.status.idle":"2024-05-02T09:10:04.803734Z","shell.execute_reply.started":"2024-05-02T09:10:04.797600Z","shell.execute_reply":"2024-05-02T09:10:04.802776Z"},"trusted":true},"execution_count":130,"outputs":[{"execution_count":130,"output_type":"execute_result","data":{"text/plain":"(111, 12000)"},"metadata":{}}]},{"cell_type":"code","source":"samples_new2 = samples_new.copy()\nsamples_new2 = samples_new2.transpose()\nnp.random.shuffle(samples_new2)\nsamples_new2 = samples_new2.transpose()","metadata":{"execution":{"iopub.status.busy":"2024-05-02T09:10:51.359538Z","iopub.execute_input":"2024-05-02T09:10:51.360417Z","iopub.status.idle":"2024-05-02T09:10:51.399231Z","shell.execute_reply.started":"2024-05-02T09:10:51.360380Z","shell.execute_reply":"2024-05-02T09:10:51.398172Z"},"trusted":true},"execution_count":135,"outputs":[]},{"cell_type":"code","source":"samples_new2","metadata":{"execution":{"iopub.status.busy":"2024-05-02T09:10:56.412938Z","iopub.execute_input":"2024-05-02T09:10:56.413942Z","iopub.status.idle":"2024-05-02T09:10:56.421322Z","shell.execute_reply.started":"2024-05-02T09:10:56.413902Z","shell.execute_reply":"2024-05-02T09:10:56.420282Z"},"trusted":true},"execution_count":137,"outputs":[{"execution_count":137,"output_type":"execute_result","data":{"text/plain":"array([[-6.2894499e-01,  2.7472367e+00,  4.9591601e-01, ...,\n         3.3758691e-01,  4.3860447e-01, -5.2227789e-01],\n       [-7.8615975e-01,  2.4988220e+00,  7.4630612e-01, ...,\n        -5.6354260e-01,  1.0224484e+00, -7.8539824e-01],\n       [ 3.7997833e-01,  3.1360981e+00,  2.3612043e-01, ...,\n        -3.5828272e-01,  1.2187922e-01, -8.0225033e-01],\n       ...,\n       [-1.1683521e+00,  3.7436658e-01, -4.5901632e-01, ...,\n        -3.3158427e-01, -5.8031994e-01, -1.2946153e+00],\n       [ 2.2954570e-01, -1.2408385e+00, -4.5528954e-01, ...,\n         5.8066189e-01,  6.3842344e-01,  1.8118092e+00],\n       [-4.4324553e-01, -1.7809411e+00, -1.5722710e+00, ...,\n        -3.0964091e-03,  7.7119803e-01, -1.5302658e-02]], dtype=float32)"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2024-05-02T09:09:21.194714Z","iopub.execute_input":"2024-05-02T09:09:21.195723Z","iopub.status.idle":"2024-05-02T09:09:21.201949Z","shell.execute_reply.started":"2024-05-02T09:09:21.195685Z","shell.execute_reply":"2024-05-02T09:09:21.200997Z"},"trusted":true},"execution_count":128,"outputs":[{"execution_count":128,"output_type":"execute_result","data":{"text/plain":"array([[5, 6],\n       [1, 0],\n       [3, 4]])"},"metadata":{}}]},{"cell_type":"code","source":"real = []\nfor i in range(12):\n    real_step = np.array(val_standard.iloc[i*111:(i+1)*111,2].tolist())\n    repeated_arr = np.repeat(real_step, 1000, axis=0)\n    two_dim_array = repeated_arr.reshape(111, 1000).transpose()\n    real.append(two_dim_array)\nreal_2 = np.concatenate(real,axis=0).transpose()\n        ","metadata":{"execution":{"iopub.status.busy":"2024-05-02T08:58:29.758014Z","iopub.execute_input":"2024-05-02T08:58:29.758838Z","iopub.status.idle":"2024-05-02T08:58:29.775891Z","shell.execute_reply.started":"2024-05-02T08:58:29.758790Z","shell.execute_reply":"2024-05-02T08:58:29.774894Z"},"trusted":true},"execution_count":109,"outputs":[]},{"cell_type":"code","source":"val_standard.tail(111)","metadata":{"execution":{"iopub.status.busy":"2024-05-02T08:57:17.641216Z","iopub.execute_input":"2024-05-02T08:57:17.642150Z","iopub.status.idle":"2024-05-02T08:57:17.656270Z","shell.execute_reply.started":"2024-05-02T08:57:17.642103Z","shell.execute_reply":"2024-05-02T08:57:17.655243Z"},"trusted":true},"execution_count":104,"outputs":[{"execution_count":104,"output_type":"execute_result","data":{"text/plain":"                     Date Node     Value  Month_Of_Year  Region  Zone  State\n1221  2015-12-01 00:00:00    T -0.218299             12       0     0      0\n1222  2015-12-01 00:00:00    A -0.373919             12       0     0      1\n1223  2015-12-01 00:00:00    B  0.061718             12       0     0      2\n1224  2015-12-01 00:00:00    C -0.628933             12       0     0      3\n1225  2015-12-01 00:00:00    D  0.231343             12       0     0      4\n...                   ...  ...       ...            ...     ...   ...    ...\n1327  2015-12-01 00:00:00  GAC -0.763644             12      72    26      7\n1328  2015-12-01 00:00:00  GBA -0.168195             12      73    27      7\n1329  2015-12-01 00:00:00  GBB -1.302586             12      74    27      7\n1330  2015-12-01 00:00:00  GBC  0.297599             12      75    27      7\n1331  2015-12-01 00:00:00  GBD  0.463693             12      76    27      7\n\n[111 rows x 7 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Date</th>\n      <th>Node</th>\n      <th>Value</th>\n      <th>Month_Of_Year</th>\n      <th>Region</th>\n      <th>Zone</th>\n      <th>State</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1221</th>\n      <td>2015-12-01 00:00:00</td>\n      <td>T</td>\n      <td>-0.218299</td>\n      <td>12</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1222</th>\n      <td>2015-12-01 00:00:00</td>\n      <td>A</td>\n      <td>-0.373919</td>\n      <td>12</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1223</th>\n      <td>2015-12-01 00:00:00</td>\n      <td>B</td>\n      <td>0.061718</td>\n      <td>12</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>1224</th>\n      <td>2015-12-01 00:00:00</td>\n      <td>C</td>\n      <td>-0.628933</td>\n      <td>12</td>\n      <td>0</td>\n      <td>0</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>1225</th>\n      <td>2015-12-01 00:00:00</td>\n      <td>D</td>\n      <td>0.231343</td>\n      <td>12</td>\n      <td>0</td>\n      <td>0</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1327</th>\n      <td>2015-12-01 00:00:00</td>\n      <td>GAC</td>\n      <td>-0.763644</td>\n      <td>12</td>\n      <td>72</td>\n      <td>26</td>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>1328</th>\n      <td>2015-12-01 00:00:00</td>\n      <td>GBA</td>\n      <td>-0.168195</td>\n      <td>12</td>\n      <td>73</td>\n      <td>27</td>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>1329</th>\n      <td>2015-12-01 00:00:00</td>\n      <td>GBB</td>\n      <td>-1.302586</td>\n      <td>12</td>\n      <td>74</td>\n      <td>27</td>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>1330</th>\n      <td>2015-12-01 00:00:00</td>\n      <td>GBC</td>\n      <td>0.297599</td>\n      <td>12</td>\n      <td>75</td>\n      <td>27</td>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>1331</th>\n      <td>2015-12-01 00:00:00</td>\n      <td>GBD</td>\n      <td>0.463693</td>\n      <td>12</td>\n      <td>76</td>\n      <td>27</td>\n      <td>7</td>\n    </tr>\n  </tbody>\n</table>\n<p>111 rows × 7 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"real_2.shape","metadata":{"execution":{"iopub.status.busy":"2024-05-02T09:00:51.261954Z","iopub.execute_input":"2024-05-02T09:00:51.262815Z","iopub.status.idle":"2024-05-02T09:00:51.268589Z","shell.execute_reply.started":"2024-05-02T09:00:51.262778Z","shell.execute_reply":"2024-05-02T09:00:51.267675Z"},"trusted":true},"execution_count":113,"outputs":[{"execution_count":113,"output_type":"execute_result","data":{"text/plain":"(111, 12000)"},"metadata":{}}]},{"cell_type":"code","source":"\nclass DeepARTuningObjective:\n    def __init__(\n        self, dataset, prediction_length, freq, validation_label,index_list\n    ):\n        self.dataset = dataset\n        self.prediction_length = prediction_length\n        self.freq = freq\n        self.validation_label=validation_label\n\n\n    def get_params(self, trial) -> dict:\n        return {\n            \"num_layers\": trial.suggest_int(\"num_layers\", 2, 5),\n            \"hidden_size\": trial.suggest_int(\"hidden_size\", 20, 50),\n            \"lr\":trial.suggest_categorical('lr', choices=[1e-4,1e-3,1e-2]),\n            \"batch_size\": trial.suggest_int(\"batch_size\", low= 16, high =64, step =16)\n        }\n\n    def __call__(self, trial):\n        params = self.get_params(trial)\n        estimator = DeepAREstimator(\n            num_layers=params[\"num_layers\"],\n            hidden_size=params[\"hidden_size\"],\n            lr=params[\"lr\"],\n            prediction_length=self.prediction_length,\n            context_length=10*prediction_length,\n            patience = 10,\n            freq=self.freq,\n            distr_output=NormalOutput(),\n            scaling=False,\n            num_parallel_samples=1000,\n            batch_size=params[\"batch_size\"],\n            trainer_kwargs={\n                \"max_epochs\": 20,\n            },\n        )\n\n        predictor = estimator.train(self.dataset, cache_data=True)\n        forecast_it = predictor.predict(self.dataset)\n\n        forecasts = list(forecast_it)\n        \n        samples_total = []\n        for j in range(12):\n            samples_step = []\n            for i in range(len(forecasts)):\n                samples_step.append(forecasts[i].samples[:,j].reshape((1,1000)))\n            samples_1 = []\n            for k in index_list:\n                samples_1.append(samples_step[k])\n            samples_total.append(np.concatenate(samples_1,axis=0))\n        samples_new = np.concatenate(samples_total,axis=1)\n        \n        real = []\n        for i in range(12):\n            real_step = np.array(val_standard.iloc[i*111:(i+1)*111,2].tolist())\n            repeated_arr = np.repeat(real_step, 1000, axis=0)\n            two_dim_array = repeated_arr.reshape(111, 1000).transpose()\n            real.append(two_dim_array)\n        real_2 = np.concatenate(real,axis=0).transpose()\n        \n        samples_new2 = samples_new.copy()\n        samples_new2 = samples_new2.transpose()\n        np.random.shuffle(samples_new2)\n        samples_new2 = samples_new2.transpose()\n        \n        dif1 = samples_new - samples_new2\n        dif2 = real_2 - samples_new\n        term1 = np.sum(np.square(dif1))\n        term2 = np.sum(np.square(dif2))\n        res1 = ((-0.5*term1)+term2)/(real_2.shape[1])\n        \n        return res1","metadata":{"execution":{"iopub.status.busy":"2024-05-02T09:13:29.347903Z","iopub.execute_input":"2024-05-02T09:13:29.348879Z","iopub.status.idle":"2024-05-02T09:13:29.366559Z","shell.execute_reply.started":"2024-05-02T09:13:29.348836Z","shell.execute_reply":"2024-05-02T09:13:29.365450Z"},"trusted":true},"execution_count":141,"outputs":[]},{"cell_type":"code","source":"import time\nimport optuna\n\nstart_time = time.time()\nstudy = optuna.create_study(direction=\"minimize\")\nstudy.optimize(\n    DeepARTuningObjective(\n        train_ds, prediction_length, freq,val_standard, index_list\n    ),\n    n_trials=10,\n)\n\nprint(\"Number of finished trials: {}\".format(len(study.trials)))\n\nprint(\"Best trial:\")\ntrial = study.best_trial\n\nprint(\"  Value: {}\".format(trial.value))\n\nprint(\"  Params: \")\nfor key, value in trial.params.items():\n    print(\"    {}: {}\".format(key, value))\nprint(time.time() - start_time)","metadata":{"execution":{"iopub.status.busy":"2024-05-02T09:13:32.921667Z","iopub.execute_input":"2024-05-02T09:13:32.922306Z","iopub.status.idle":"2024-05-02T09:16:35.748180Z","shell.execute_reply.started":"2024-05-02T09:13:32.922272Z","shell.execute_reply":"2024-05-02T09:16:35.747267Z"},"trusted":true},"execution_count":142,"outputs":[{"name":"stderr","text":"[I 2024-05-02 09:13:32,926] A new study created in memory with name: no-name-0c32630b-bf9e-45db-b731-e9bce4e161af\nINFO: GPU available: True (cuda), used: True\nINFO: TPU available: False, using: 0 TPU cores\nINFO: IPU available: False, using: 0 IPUs\nINFO: HPU available: False, using: 0 HPUs\n/opt/conda/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:74: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\nINFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\nINFO: \n  | Name  | Type        | Params | In sizes                                                      | Out sizes    \n----------------------------------------------------------------------------------------------------------------------\n0 | model | DeepARModel | 13.1 K | [[1, 1], [1, 1], [1, 156, 2], [1, 156], [1, 156], [1, 12, 2]] | [1, 1000, 12]\n----------------------------------------------------------------------------------------------------------------------\n13.1 K    Trainable params\n0         Non-trainable params\n13.1 K    Total params\n0.052     Total estimated model params size (MB)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"60119ea24b8d47e3b1356b89b583113a"}},"metadata":{}},{"name":"stderr","text":"INFO: Epoch 0, global step 50: 'train_loss' reached 1.08082 (best 1.08082), saving model to '/kaggle/working/lightning_logs/version_1/checkpoints/epoch=0-step=50.ckpt' as top 1\nINFO: Epoch 1, global step 100: 'train_loss' reached 1.04635 (best 1.04635), saving model to '/kaggle/working/lightning_logs/version_1/checkpoints/epoch=1-step=100.ckpt' as top 1\nINFO: Epoch 2, global step 150: 'train_loss' reached 1.03543 (best 1.03543), saving model to '/kaggle/working/lightning_logs/version_1/checkpoints/epoch=2-step=150.ckpt' as top 1\nINFO: Epoch 3, global step 200: 'train_loss' reached 1.01736 (best 1.01736), saving model to '/kaggle/working/lightning_logs/version_1/checkpoints/epoch=3-step=200.ckpt' as top 1\nINFO: Epoch 4, global step 250: 'train_loss' reached 1.00309 (best 1.00309), saving model to '/kaggle/working/lightning_logs/version_1/checkpoints/epoch=4-step=250.ckpt' as top 1\nINFO: Epoch 5, global step 300: 'train_loss' reached 0.98351 (best 0.98351), saving model to '/kaggle/working/lightning_logs/version_1/checkpoints/epoch=5-step=300.ckpt' as top 1\nINFO: Epoch 6, global step 350: 'train_loss' reached 0.95744 (best 0.95744), saving model to '/kaggle/working/lightning_logs/version_1/checkpoints/epoch=6-step=350.ckpt' as top 1\nINFO: Epoch 7, global step 400: 'train_loss' reached 0.93985 (best 0.93985), saving model to '/kaggle/working/lightning_logs/version_1/checkpoints/epoch=7-step=400.ckpt' as top 1\nINFO: Epoch 8, global step 450: 'train_loss' was not in top 1\nINFO: Epoch 9, global step 500: 'train_loss' reached 0.92224 (best 0.92224), saving model to '/kaggle/working/lightning_logs/version_1/checkpoints/epoch=9-step=500.ckpt' as top 1\nINFO: Epoch 10, global step 550: 'train_loss' reached 0.87546 (best 0.87546), saving model to '/kaggle/working/lightning_logs/version_1/checkpoints/epoch=10-step=550.ckpt' as top 1\nINFO: Epoch 11, global step 600: 'train_loss' was not in top 1\nINFO: Epoch 12, global step 650: 'train_loss' reached 0.86239 (best 0.86239), saving model to '/kaggle/working/lightning_logs/version_1/checkpoints/epoch=12-step=650.ckpt' as top 1\nINFO: Epoch 13, global step 700: 'train_loss' was not in top 1\nINFO: Epoch 14, global step 750: 'train_loss' reached 0.85487 (best 0.85487), saving model to '/kaggle/working/lightning_logs/version_1/checkpoints/epoch=14-step=750.ckpt' as top 1\nINFO: Epoch 15, global step 800: 'train_loss' was not in top 1\nINFO: Epoch 16, global step 850: 'train_loss' was not in top 1\nINFO: Epoch 17, global step 900: 'train_loss' reached 0.84583 (best 0.84583), saving model to '/kaggle/working/lightning_logs/version_1/checkpoints/epoch=17-step=900.ckpt' as top 1\nINFO: Epoch 18, global step 950: 'train_loss' was not in top 1\nINFO: Epoch 19, global step 1000: 'train_loss' reached 0.83300 (best 0.83300), saving model to '/kaggle/working/lightning_logs/version_1/checkpoints/epoch=19-step=1000.ckpt' as top 1\nINFO: `Trainer.fit` stopped: `max_epochs=20` reached.\n[I 2024-05-02 09:13:47,634] Trial 0 finished with value: 43.790286586687685 and parameters: {'num_layers': 2, 'hidden_size': 29, 'lr': 0.0001, 'batch_size': 16}. Best is trial 0 with value: 43.790286586687685.\nINFO: GPU available: True (cuda), used: True\nINFO: TPU available: False, using: 0 TPU cores\nINFO: IPU available: False, using: 0 IPUs\nINFO: HPU available: False, using: 0 HPUs\n/opt/conda/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:74: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\nINFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\nINFO: \n  | Name  | Type        | Params | In sizes                                                      | Out sizes    \n----------------------------------------------------------------------------------------------------------------------\n0 | model | DeepARModel | 38.1 K | [[1, 1], [1, 1], [1, 156, 2], [1, 156], [1, 156], [1, 12, 2]] | [1, 1000, 12]\n----------------------------------------------------------------------------------------------------------------------\n38.1 K    Trainable params\n0         Non-trainable params\n38.1 K    Total params\n0.153     Total estimated model params size (MB)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8a1171e5685d42738544c895cfabec0d"}},"metadata":{}},{"name":"stderr","text":"INFO: Epoch 0, global step 50: 'train_loss' reached 0.92368 (best 0.92368), saving model to '/kaggle/working/lightning_logs/version_2/checkpoints/epoch=0-step=50.ckpt' as top 1\nINFO: Epoch 1, global step 100: 'train_loss' reached 0.79145 (best 0.79145), saving model to '/kaggle/working/lightning_logs/version_2/checkpoints/epoch=1-step=100.ckpt' as top 1\nINFO: Epoch 2, global step 150: 'train_loss' reached 0.73152 (best 0.73152), saving model to '/kaggle/working/lightning_logs/version_2/checkpoints/epoch=2-step=150.ckpt' as top 1\nINFO: Epoch 3, global step 200: 'train_loss' reached 0.71114 (best 0.71114), saving model to '/kaggle/working/lightning_logs/version_2/checkpoints/epoch=3-step=200.ckpt' as top 1\nINFO: Epoch 4, global step 250: 'train_loss' reached 0.65685 (best 0.65685), saving model to '/kaggle/working/lightning_logs/version_2/checkpoints/epoch=4-step=250.ckpt' as top 1\nINFO: Epoch 5, global step 300: 'train_loss' reached 0.64084 (best 0.64084), saving model to '/kaggle/working/lightning_logs/version_2/checkpoints/epoch=5-step=300.ckpt' as top 1\nINFO: Epoch 6, global step 350: 'train_loss' reached 0.60258 (best 0.60258), saving model to '/kaggle/working/lightning_logs/version_2/checkpoints/epoch=6-step=350.ckpt' as top 1\nINFO: Epoch 7, global step 400: 'train_loss' reached 0.58495 (best 0.58495), saving model to '/kaggle/working/lightning_logs/version_2/checkpoints/epoch=7-step=400.ckpt' as top 1\nINFO: Epoch 8, global step 450: 'train_loss' reached 0.54280 (best 0.54280), saving model to '/kaggle/working/lightning_logs/version_2/checkpoints/epoch=8-step=450.ckpt' as top 1\nINFO: Epoch 9, global step 500: 'train_loss' reached 0.53467 (best 0.53467), saving model to '/kaggle/working/lightning_logs/version_2/checkpoints/epoch=9-step=500.ckpt' as top 1\nINFO: Epoch 10, global step 550: 'train_loss' was not in top 1\nINFO: Epoch 11, global step 600: 'train_loss' reached 0.50742 (best 0.50742), saving model to '/kaggle/working/lightning_logs/version_2/checkpoints/epoch=11-step=600.ckpt' as top 1\nINFO: Epoch 12, global step 650: 'train_loss' reached 0.46143 (best 0.46143), saving model to '/kaggle/working/lightning_logs/version_2/checkpoints/epoch=12-step=650.ckpt' as top 1\nINFO: Epoch 13, global step 700: 'train_loss' reached 0.43207 (best 0.43207), saving model to '/kaggle/working/lightning_logs/version_2/checkpoints/epoch=13-step=700.ckpt' as top 1\nINFO: Epoch 14, global step 750: 'train_loss' was not in top 1\nINFO: Epoch 15, global step 800: 'train_loss' reached 0.42478 (best 0.42478), saving model to '/kaggle/working/lightning_logs/version_2/checkpoints/epoch=15-step=800.ckpt' as top 1\nINFO: Epoch 16, global step 850: 'train_loss' reached 0.38609 (best 0.38609), saving model to '/kaggle/working/lightning_logs/version_2/checkpoints/epoch=16-step=850.ckpt' as top 1\nINFO: Epoch 17, global step 900: 'train_loss' reached 0.36615 (best 0.36615), saving model to '/kaggle/working/lightning_logs/version_2/checkpoints/epoch=17-step=900.ckpt' as top 1\nINFO: Epoch 18, global step 950: 'train_loss' reached 0.35908 (best 0.35908), saving model to '/kaggle/working/lightning_logs/version_2/checkpoints/epoch=18-step=950.ckpt' as top 1\nINFO: Epoch 19, global step 1000: 'train_loss' reached 0.35816 (best 0.35816), saving model to '/kaggle/working/lightning_logs/version_2/checkpoints/epoch=19-step=1000.ckpt' as top 1\nINFO: `Trainer.fit` stopped: `max_epochs=20` reached.\n[I 2024-05-02 09:14:03,099] Trial 1 finished with value: 10.073001049791909 and parameters: {'num_layers': 3, 'hidden_size': 41, 'lr': 0.01, 'batch_size': 16}. Best is trial 1 with value: 10.073001049791909.\nINFO: GPU available: True (cuda), used: True\nINFO: TPU available: False, using: 0 TPU cores\nINFO: IPU available: False, using: 0 IPUs\nINFO: HPU available: False, using: 0 HPUs\n/opt/conda/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:74: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\nINFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\nINFO: \n  | Name  | Type        | Params | In sizes                                                      | Out sizes    \n----------------------------------------------------------------------------------------------------------------------\n0 | model | DeepARModel | 25.4 K | [[1, 1], [1, 1], [1, 156, 2], [1, 156], [1, 156], [1, 12, 2]] | [1, 1000, 12]\n----------------------------------------------------------------------------------------------------------------------\n25.4 K    Trainable params\n0         Non-trainable params\n25.4 K    Total params\n0.102     Total estimated model params size (MB)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"27eed8a05bf848b68cdd7989475733f3"}},"metadata":{}},{"name":"stderr","text":"INFO: Epoch 0, global step 50: 'train_loss' reached 1.08963 (best 1.08963), saving model to '/kaggle/working/lightning_logs/version_3/checkpoints/epoch=0-step=50.ckpt' as top 1\nINFO: Epoch 1, global step 100: 'train_loss' reached 1.08576 (best 1.08576), saving model to '/kaggle/working/lightning_logs/version_3/checkpoints/epoch=1-step=100.ckpt' as top 1\nINFO: Epoch 2, global step 150: 'train_loss' reached 1.02589 (best 1.02589), saving model to '/kaggle/working/lightning_logs/version_3/checkpoints/epoch=2-step=150.ckpt' as top 1\nINFO: Epoch 3, global step 200: 'train_loss' reached 1.02390 (best 1.02390), saving model to '/kaggle/working/lightning_logs/version_3/checkpoints/epoch=3-step=200.ckpt' as top 1\nINFO: Epoch 4, global step 250: 'train_loss' reached 0.99125 (best 0.99125), saving model to '/kaggle/working/lightning_logs/version_3/checkpoints/epoch=4-step=250.ckpt' as top 1\nINFO: Epoch 5, global step 300: 'train_loss' reached 0.96952 (best 0.96952), saving model to '/kaggle/working/lightning_logs/version_3/checkpoints/epoch=5-step=300.ckpt' as top 1\nINFO: Epoch 6, global step 350: 'train_loss' reached 0.94990 (best 0.94990), saving model to '/kaggle/working/lightning_logs/version_3/checkpoints/epoch=6-step=350.ckpt' as top 1\nINFO: Epoch 7, global step 400: 'train_loss' reached 0.92933 (best 0.92933), saving model to '/kaggle/working/lightning_logs/version_3/checkpoints/epoch=7-step=400.ckpt' as top 1\nINFO: Epoch 8, global step 450: 'train_loss' reached 0.89937 (best 0.89937), saving model to '/kaggle/working/lightning_logs/version_3/checkpoints/epoch=8-step=450.ckpt' as top 1\nINFO: Epoch 9, global step 500: 'train_loss' was not in top 1\nINFO: Epoch 10, global step 550: 'train_loss' was not in top 1\nINFO: Epoch 11, global step 600: 'train_loss' reached 0.89228 (best 0.89228), saving model to '/kaggle/working/lightning_logs/version_3/checkpoints/epoch=11-step=600.ckpt' as top 1\nINFO: Epoch 12, global step 650: 'train_loss' was not in top 1\nINFO: Epoch 13, global step 700: 'train_loss' reached 0.87878 (best 0.87878), saving model to '/kaggle/working/lightning_logs/version_3/checkpoints/epoch=13-step=700.ckpt' as top 1\nINFO: Epoch 14, global step 750: 'train_loss' reached 0.87540 (best 0.87540), saving model to '/kaggle/working/lightning_logs/version_3/checkpoints/epoch=14-step=750.ckpt' as top 1\nINFO: Epoch 15, global step 800: 'train_loss' reached 0.85480 (best 0.85480), saving model to '/kaggle/working/lightning_logs/version_3/checkpoints/epoch=15-step=800.ckpt' as top 1\nINFO: Epoch 16, global step 850: 'train_loss' reached 0.84883 (best 0.84883), saving model to '/kaggle/working/lightning_logs/version_3/checkpoints/epoch=16-step=850.ckpt' as top 1\nINFO: Epoch 17, global step 900: 'train_loss' reached 0.83375 (best 0.83375), saving model to '/kaggle/working/lightning_logs/version_3/checkpoints/epoch=17-step=900.ckpt' as top 1\nINFO: Epoch 18, global step 950: 'train_loss' reached 0.81930 (best 0.81930), saving model to '/kaggle/working/lightning_logs/version_3/checkpoints/epoch=18-step=950.ckpt' as top 1\nINFO: Epoch 19, global step 1000: 'train_loss' was not in top 1\nINFO: `Trainer.fit` stopped: `max_epochs=20` reached.\n[I 2024-05-02 09:14:21,637] Trial 2 finished with value: 42.09023105395527 and parameters: {'num_layers': 3, 'hidden_size': 33, 'lr': 0.0001, 'batch_size': 32}. Best is trial 1 with value: 10.073001049791909.\nINFO: GPU available: True (cuda), used: True\nINFO: TPU available: False, using: 0 TPU cores\nINFO: IPU available: False, using: 0 IPUs\nINFO: HPU available: False, using: 0 HPUs\n/opt/conda/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:74: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\nINFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\nINFO: \n  | Name  | Type        | Params | In sizes                                                      | Out sizes    \n----------------------------------------------------------------------------------------------------------------------\n0 | model | DeepARModel | 45.9 K | [[1, 1], [1, 1], [1, 156, 2], [1, 156], [1, 156], [1, 12, 2]] | [1, 1000, 12]\n----------------------------------------------------------------------------------------------------------------------\n45.9 K    Trainable params\n0         Non-trainable params\n45.9 K    Total params\n0.184     Total estimated model params size (MB)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2a88089963e645a9b309eb2a501ccc75"}},"metadata":{}},{"name":"stderr","text":"INFO: Epoch 0, global step 50: 'train_loss' reached 0.99461 (best 0.99461), saving model to '/kaggle/working/lightning_logs/version_4/checkpoints/epoch=0-step=50.ckpt' as top 1\nINFO: Epoch 1, global step 100: 'train_loss' reached 0.84626 (best 0.84626), saving model to '/kaggle/working/lightning_logs/version_4/checkpoints/epoch=1-step=100.ckpt' as top 1\nINFO: Epoch 2, global step 150: 'train_loss' reached 0.74870 (best 0.74870), saving model to '/kaggle/working/lightning_logs/version_4/checkpoints/epoch=2-step=150.ckpt' as top 1\nINFO: Epoch 3, global step 200: 'train_loss' reached 0.69406 (best 0.69406), saving model to '/kaggle/working/lightning_logs/version_4/checkpoints/epoch=3-step=200.ckpt' as top 1\nINFO: Epoch 4, global step 250: 'train_loss' reached 0.65365 (best 0.65365), saving model to '/kaggle/working/lightning_logs/version_4/checkpoints/epoch=4-step=250.ckpt' as top 1\nINFO: Epoch 5, global step 300: 'train_loss' reached 0.62810 (best 0.62810), saving model to '/kaggle/working/lightning_logs/version_4/checkpoints/epoch=5-step=300.ckpt' as top 1\nINFO: Epoch 6, global step 350: 'train_loss' reached 0.58290 (best 0.58290), saving model to '/kaggle/working/lightning_logs/version_4/checkpoints/epoch=6-step=350.ckpt' as top 1\nINFO: Epoch 7, global step 400: 'train_loss' reached 0.55472 (best 0.55472), saving model to '/kaggle/working/lightning_logs/version_4/checkpoints/epoch=7-step=400.ckpt' as top 1\nINFO: Epoch 8, global step 450: 'train_loss' reached 0.52473 (best 0.52473), saving model to '/kaggle/working/lightning_logs/version_4/checkpoints/epoch=8-step=450.ckpt' as top 1\nINFO: Epoch 9, global step 500: 'train_loss' reached 0.49151 (best 0.49151), saving model to '/kaggle/working/lightning_logs/version_4/checkpoints/epoch=9-step=500.ckpt' as top 1\nINFO: Epoch 10, global step 550: 'train_loss' reached 0.47566 (best 0.47566), saving model to '/kaggle/working/lightning_logs/version_4/checkpoints/epoch=10-step=550.ckpt' as top 1\nINFO: Epoch 11, global step 600: 'train_loss' reached 0.46244 (best 0.46244), saving model to '/kaggle/working/lightning_logs/version_4/checkpoints/epoch=11-step=600.ckpt' as top 1\nINFO: Epoch 12, global step 650: 'train_loss' reached 0.44207 (best 0.44207), saving model to '/kaggle/working/lightning_logs/version_4/checkpoints/epoch=12-step=650.ckpt' as top 1\nINFO: Epoch 13, global step 700: 'train_loss' reached 0.41957 (best 0.41957), saving model to '/kaggle/working/lightning_logs/version_4/checkpoints/epoch=13-step=700.ckpt' as top 1\nINFO: Epoch 14, global step 750: 'train_loss' reached 0.39073 (best 0.39073), saving model to '/kaggle/working/lightning_logs/version_4/checkpoints/epoch=14-step=750.ckpt' as top 1\nINFO: Epoch 15, global step 800: 'train_loss' reached 0.38171 (best 0.38171), saving model to '/kaggle/working/lightning_logs/version_4/checkpoints/epoch=15-step=800.ckpt' as top 1\nINFO: Epoch 16, global step 850: 'train_loss' reached 0.37009 (best 0.37009), saving model to '/kaggle/working/lightning_logs/version_4/checkpoints/epoch=16-step=850.ckpt' as top 1\nINFO: Epoch 17, global step 900: 'train_loss' reached 0.35583 (best 0.35583), saving model to '/kaggle/working/lightning_logs/version_4/checkpoints/epoch=17-step=900.ckpt' as top 1\nINFO: Epoch 18, global step 950: 'train_loss' reached 0.35003 (best 0.35003), saving model to '/kaggle/working/lightning_logs/version_4/checkpoints/epoch=18-step=950.ckpt' as top 1\nINFO: Epoch 19, global step 1000: 'train_loss' reached 0.33363 (best 0.33363), saving model to '/kaggle/working/lightning_logs/version_4/checkpoints/epoch=19-step=1000.ckpt' as top 1\nINFO: `Trainer.fit` stopped: `max_epochs=20` reached.\n[I 2024-05-02 09:14:46,957] Trial 3 finished with value: 12.482024692140422 and parameters: {'num_layers': 5, 'hidden_size': 34, 'lr': 0.01, 'batch_size': 64}. Best is trial 1 with value: 10.073001049791909.\nINFO: GPU available: True (cuda), used: True\nINFO: TPU available: False, using: 0 TPU cores\nINFO: IPU available: False, using: 0 IPUs\nINFO: HPU available: False, using: 0 HPUs\n/opt/conda/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:74: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\nINFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\nINFO: \n  | Name  | Type        | Params | In sizes                                                      | Out sizes    \n----------------------------------------------------------------------------------------------------------------------\n0 | model | DeepARModel | 18.8 K | [[1, 1], [1, 1], [1, 156, 2], [1, 156], [1, 156], [1, 12, 2]] | [1, 1000, 12]\n----------------------------------------------------------------------------------------------------------------------\n18.8 K    Trainable params\n0         Non-trainable params\n18.8 K    Total params\n0.075     Total estimated model params size (MB)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b62d589ea7ae4d1988a5b8e90d46aed6"}},"metadata":{}},{"name":"stderr","text":"INFO: Epoch 0, global step 50: 'train_loss' reached 1.06671 (best 1.06671), saving model to '/kaggle/working/lightning_logs/version_5/checkpoints/epoch=0-step=50.ckpt' as top 1\nINFO: Epoch 1, global step 100: 'train_loss' reached 0.94490 (best 0.94490), saving model to '/kaggle/working/lightning_logs/version_5/checkpoints/epoch=1-step=100.ckpt' as top 1\nINFO: Epoch 2, global step 150: 'train_loss' reached 0.86797 (best 0.86797), saving model to '/kaggle/working/lightning_logs/version_5/checkpoints/epoch=2-step=150.ckpt' as top 1\nINFO: Epoch 3, global step 200: 'train_loss' reached 0.84294 (best 0.84294), saving model to '/kaggle/working/lightning_logs/version_5/checkpoints/epoch=3-step=200.ckpt' as top 1\nINFO: Epoch 4, global step 250: 'train_loss' reached 0.80209 (best 0.80209), saving model to '/kaggle/working/lightning_logs/version_5/checkpoints/epoch=4-step=250.ckpt' as top 1\nINFO: Epoch 5, global step 300: 'train_loss' reached 0.77007 (best 0.77007), saving model to '/kaggle/working/lightning_logs/version_5/checkpoints/epoch=5-step=300.ckpt' as top 1\nINFO: Epoch 6, global step 350: 'train_loss' reached 0.74532 (best 0.74532), saving model to '/kaggle/working/lightning_logs/version_5/checkpoints/epoch=6-step=350.ckpt' as top 1\nINFO: Epoch 7, global step 400: 'train_loss' reached 0.73910 (best 0.73910), saving model to '/kaggle/working/lightning_logs/version_5/checkpoints/epoch=7-step=400.ckpt' as top 1\nINFO: Epoch 8, global step 450: 'train_loss' reached 0.71958 (best 0.71958), saving model to '/kaggle/working/lightning_logs/version_5/checkpoints/epoch=8-step=450.ckpt' as top 1\nINFO: Epoch 9, global step 500: 'train_loss' reached 0.71802 (best 0.71802), saving model to '/kaggle/working/lightning_logs/version_5/checkpoints/epoch=9-step=500.ckpt' as top 1\nINFO: Epoch 10, global step 550: 'train_loss' reached 0.71188 (best 0.71188), saving model to '/kaggle/working/lightning_logs/version_5/checkpoints/epoch=10-step=550.ckpt' as top 1\nINFO: Epoch 11, global step 600: 'train_loss' reached 0.69941 (best 0.69941), saving model to '/kaggle/working/lightning_logs/version_5/checkpoints/epoch=11-step=600.ckpt' as top 1\nINFO: Epoch 12, global step 650: 'train_loss' reached 0.69495 (best 0.69495), saving model to '/kaggle/working/lightning_logs/version_5/checkpoints/epoch=12-step=650.ckpt' as top 1\nINFO: Epoch 13, global step 700: 'train_loss' reached 0.69287 (best 0.69287), saving model to '/kaggle/working/lightning_logs/version_5/checkpoints/epoch=13-step=700.ckpt' as top 1\nINFO: Epoch 14, global step 750: 'train_loss' reached 0.67725 (best 0.67725), saving model to '/kaggle/working/lightning_logs/version_5/checkpoints/epoch=14-step=750.ckpt' as top 1\nINFO: Epoch 15, global step 800: 'train_loss' was not in top 1\nINFO: Epoch 16, global step 850: 'train_loss' reached 0.66739 (best 0.66739), saving model to '/kaggle/working/lightning_logs/version_5/checkpoints/epoch=16-step=850.ckpt' as top 1\nINFO: Epoch 17, global step 900: 'train_loss' reached 0.65461 (best 0.65461), saving model to '/kaggle/working/lightning_logs/version_5/checkpoints/epoch=17-step=900.ckpt' as top 1\nINFO: Epoch 18, global step 950: 'train_loss' reached 0.64498 (best 0.64498), saving model to '/kaggle/working/lightning_logs/version_5/checkpoints/epoch=18-step=950.ckpt' as top 1\nINFO: Epoch 19, global step 1000: 'train_loss' was not in top 1\nINFO: `Trainer.fit` stopped: `max_epochs=20` reached.\n[I 2024-05-02 09:15:11,075] Trial 4 finished with value: 14.287975010704423 and parameters: {'num_layers': 3, 'hidden_size': 28, 'lr': 0.001, 'batch_size': 64}. Best is trial 1 with value: 10.073001049791909.\nINFO: GPU available: True (cuda), used: True\nINFO: TPU available: False, using: 0 TPU cores\nINFO: IPU available: False, using: 0 IPUs\nINFO: HPU available: False, using: 0 HPUs\n/opt/conda/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:74: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\nINFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\nINFO: \n  | Name  | Type        | Params | In sizes                                                      | Out sizes    \n----------------------------------------------------------------------------------------------------------------------\n0 | model | DeepARModel | 45.5 K | [[1, 1], [1, 1], [1, 156, 2], [1, 156], [1, 156], [1, 12, 2]] | [1, 1000, 12]\n----------------------------------------------------------------------------------------------------------------------\n45.5 K    Trainable params\n0         Non-trainable params\n45.5 K    Total params\n0.182     Total estimated model params size (MB)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"abb1294e4c3949a699c619c8ca7aac97"}},"metadata":{}},{"name":"stderr","text":"INFO: Epoch 0, global step 50: 'train_loss' reached 1.05734 (best 1.05734), saving model to '/kaggle/working/lightning_logs/version_6/checkpoints/epoch=0-step=50.ckpt' as top 1\nINFO: Epoch 1, global step 100: 'train_loss' reached 1.03622 (best 1.03622), saving model to '/kaggle/working/lightning_logs/version_6/checkpoints/epoch=1-step=100.ckpt' as top 1\nINFO: Epoch 2, global step 150: 'train_loss' reached 1.02194 (best 1.02194), saving model to '/kaggle/working/lightning_logs/version_6/checkpoints/epoch=2-step=150.ckpt' as top 1\nINFO: Epoch 3, global step 200: 'train_loss' reached 1.00565 (best 1.00565), saving model to '/kaggle/working/lightning_logs/version_6/checkpoints/epoch=3-step=200.ckpt' as top 1\nINFO: Epoch 4, global step 250: 'train_loss' reached 0.99141 (best 0.99141), saving model to '/kaggle/working/lightning_logs/version_6/checkpoints/epoch=4-step=250.ckpt' as top 1\nINFO: Epoch 5, global step 300: 'train_loss' reached 0.98464 (best 0.98464), saving model to '/kaggle/working/lightning_logs/version_6/checkpoints/epoch=5-step=300.ckpt' as top 1\nINFO: Epoch 6, global step 350: 'train_loss' reached 0.93022 (best 0.93022), saving model to '/kaggle/working/lightning_logs/version_6/checkpoints/epoch=6-step=350.ckpt' as top 1\nINFO: Epoch 7, global step 400: 'train_loss' reached 0.92792 (best 0.92792), saving model to '/kaggle/working/lightning_logs/version_6/checkpoints/epoch=7-step=400.ckpt' as top 1\nINFO: Epoch 8, global step 450: 'train_loss' reached 0.91880 (best 0.91880), saving model to '/kaggle/working/lightning_logs/version_6/checkpoints/epoch=8-step=450.ckpt' as top 1\nINFO: Epoch 9, global step 500: 'train_loss' reached 0.89821 (best 0.89821), saving model to '/kaggle/working/lightning_logs/version_6/checkpoints/epoch=9-step=500.ckpt' as top 1\nINFO: Epoch 10, global step 550: 'train_loss' was not in top 1\nINFO: Epoch 11, global step 600: 'train_loss' was not in top 1\nINFO: Epoch 12, global step 650: 'train_loss' reached 0.88674 (best 0.88674), saving model to '/kaggle/working/lightning_logs/version_6/checkpoints/epoch=12-step=650.ckpt' as top 1\nINFO: Epoch 13, global step 700: 'train_loss' reached 0.87904 (best 0.87904), saving model to '/kaggle/working/lightning_logs/version_6/checkpoints/epoch=13-step=700.ckpt' as top 1\nINFO: Epoch 14, global step 750: 'train_loss' was not in top 1\nINFO: Epoch 15, global step 800: 'train_loss' reached 0.87148 (best 0.87148), saving model to '/kaggle/working/lightning_logs/version_6/checkpoints/epoch=15-step=800.ckpt' as top 1\nINFO: Epoch 16, global step 850: 'train_loss' reached 0.85692 (best 0.85692), saving model to '/kaggle/working/lightning_logs/version_6/checkpoints/epoch=16-step=850.ckpt' as top 1\nINFO: Epoch 17, global step 900: 'train_loss' reached 0.84920 (best 0.84920), saving model to '/kaggle/working/lightning_logs/version_6/checkpoints/epoch=17-step=900.ckpt' as top 1\nINFO: Epoch 18, global step 950: 'train_loss' was not in top 1\nINFO: Epoch 19, global step 1000: 'train_loss' was not in top 1\nINFO: `Trainer.fit` stopped: `max_epochs=20` reached.\n[I 2024-05-02 09:15:26,297] Trial 5 finished with value: 52.80284732527047 and parameters: {'num_layers': 3, 'hidden_size': 45, 'lr': 0.0001, 'batch_size': 16}. Best is trial 1 with value: 10.073001049791909.\nINFO: GPU available: True (cuda), used: True\nINFO: TPU available: False, using: 0 TPU cores\nINFO: IPU available: False, using: 0 IPUs\nINFO: HPU available: False, using: 0 HPUs\n/opt/conda/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:74: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\nINFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\nINFO: \n  | Name  | Type        | Params | In sizes                                                      | Out sizes    \n----------------------------------------------------------------------------------------------------------------------\n0 | model | DeepARModel | 65.7 K | [[1, 1], [1, 1], [1, 156, 2], [1, 156], [1, 156], [1, 12, 2]] | [1, 1000, 12]\n----------------------------------------------------------------------------------------------------------------------\n65.7 K    Trainable params\n0         Non-trainable params\n65.7 K    Total params\n0.263     Total estimated model params size (MB)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"faa091ed98c34a8a8e53d06bf0c093e4"}},"metadata":{}},{"name":"stderr","text":"INFO: Epoch 0, global step 50: 'train_loss' reached 0.95610 (best 0.95610), saving model to '/kaggle/working/lightning_logs/version_7/checkpoints/epoch=0-step=50.ckpt' as top 1\nINFO: Epoch 1, global step 100: 'train_loss' reached 0.82314 (best 0.82314), saving model to '/kaggle/working/lightning_logs/version_7/checkpoints/epoch=1-step=100.ckpt' as top 1\nINFO: Epoch 2, global step 150: 'train_loss' reached 0.75758 (best 0.75758), saving model to '/kaggle/working/lightning_logs/version_7/checkpoints/epoch=2-step=150.ckpt' as top 1\nINFO: Epoch 3, global step 200: 'train_loss' reached 0.71691 (best 0.71691), saving model to '/kaggle/working/lightning_logs/version_7/checkpoints/epoch=3-step=200.ckpt' as top 1\nINFO: Epoch 4, global step 250: 'train_loss' reached 0.67276 (best 0.67276), saving model to '/kaggle/working/lightning_logs/version_7/checkpoints/epoch=4-step=250.ckpt' as top 1\nINFO: Epoch 5, global step 300: 'train_loss' reached 0.66222 (best 0.66222), saving model to '/kaggle/working/lightning_logs/version_7/checkpoints/epoch=5-step=300.ckpt' as top 1\nINFO: Epoch 6, global step 350: 'train_loss' reached 0.59810 (best 0.59810), saving model to '/kaggle/working/lightning_logs/version_7/checkpoints/epoch=6-step=350.ckpt' as top 1\nINFO: Epoch 7, global step 400: 'train_loss' reached 0.58903 (best 0.58903), saving model to '/kaggle/working/lightning_logs/version_7/checkpoints/epoch=7-step=400.ckpt' as top 1\nINFO: Epoch 8, global step 450: 'train_loss' reached 0.53885 (best 0.53885), saving model to '/kaggle/working/lightning_logs/version_7/checkpoints/epoch=8-step=450.ckpt' as top 1\nINFO: Epoch 9, global step 500: 'train_loss' reached 0.52997 (best 0.52997), saving model to '/kaggle/working/lightning_logs/version_7/checkpoints/epoch=9-step=500.ckpt' as top 1\nINFO: Epoch 10, global step 550: 'train_loss' reached 0.50035 (best 0.50035), saving model to '/kaggle/working/lightning_logs/version_7/checkpoints/epoch=10-step=550.ckpt' as top 1\nINFO: Epoch 11, global step 600: 'train_loss' reached 0.47969 (best 0.47969), saving model to '/kaggle/working/lightning_logs/version_7/checkpoints/epoch=11-step=600.ckpt' as top 1\nINFO: Epoch 12, global step 650: 'train_loss' reached 0.44763 (best 0.44763), saving model to '/kaggle/working/lightning_logs/version_7/checkpoints/epoch=12-step=650.ckpt' as top 1\nINFO: Epoch 13, global step 700: 'train_loss' reached 0.42183 (best 0.42183), saving model to '/kaggle/working/lightning_logs/version_7/checkpoints/epoch=13-step=700.ckpt' as top 1\nINFO: Epoch 14, global step 750: 'train_loss' reached 0.40698 (best 0.40698), saving model to '/kaggle/working/lightning_logs/version_7/checkpoints/epoch=14-step=750.ckpt' as top 1\nINFO: Epoch 15, global step 800: 'train_loss' reached 0.39198 (best 0.39198), saving model to '/kaggle/working/lightning_logs/version_7/checkpoints/epoch=15-step=800.ckpt' as top 1\nINFO: Epoch 16, global step 850: 'train_loss' reached 0.35624 (best 0.35624), saving model to '/kaggle/working/lightning_logs/version_7/checkpoints/epoch=16-step=850.ckpt' as top 1\nINFO: Epoch 17, global step 900: 'train_loss' was not in top 1\nINFO: Epoch 18, global step 950: 'train_loss' reached 0.33714 (best 0.33714), saving model to '/kaggle/working/lightning_logs/version_7/checkpoints/epoch=18-step=950.ckpt' as top 1\nINFO: Epoch 19, global step 1000: 'train_loss' reached 0.33690 (best 0.33690), saving model to '/kaggle/working/lightning_logs/version_7/checkpoints/epoch=19-step=1000.ckpt' as top 1\nINFO: `Trainer.fit` stopped: `max_epochs=20` reached.\n[I 2024-05-02 09:15:45,774] Trial 6 finished with value: 16.169770572198498 and parameters: {'num_layers': 5, 'hidden_size': 41, 'lr': 0.01, 'batch_size': 32}. Best is trial 1 with value: 10.073001049791909.\nINFO: GPU available: True (cuda), used: True\nINFO: TPU available: False, using: 0 TPU cores\nINFO: IPU available: False, using: 0 IPUs\nINFO: HPU available: False, using: 0 HPUs\n/opt/conda/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:74: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\nINFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\nINFO: \n  | Name  | Type        | Params | In sizes                                                      | Out sizes    \n----------------------------------------------------------------------------------------------------------------------\n0 | model | DeepARModel | 30.6 K | [[1, 1], [1, 1], [1, 156, 2], [1, 156], [1, 156], [1, 12, 2]] | [1, 1000, 12]\n----------------------------------------------------------------------------------------------------------------------\n30.6 K    Trainable params\n0         Non-trainable params\n30.6 K    Total params\n0.122     Total estimated model params size (MB)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c1cc8f92b60f4b6f8b2e0dde28a82fd4"}},"metadata":{}},{"name":"stderr","text":"INFO: Epoch 0, global step 50: 'train_loss' reached 1.18871 (best 1.18871), saving model to '/kaggle/working/lightning_logs/version_8/checkpoints/epoch=0-step=50.ckpt' as top 1\nINFO: Epoch 1, global step 100: 'train_loss' reached 1.07974 (best 1.07974), saving model to '/kaggle/working/lightning_logs/version_8/checkpoints/epoch=1-step=100.ckpt' as top 1\nINFO: Epoch 2, global step 150: 'train_loss' reached 1.04202 (best 1.04202), saving model to '/kaggle/working/lightning_logs/version_8/checkpoints/epoch=2-step=150.ckpt' as top 1\nINFO: Epoch 3, global step 200: 'train_loss' was not in top 1\nINFO: Epoch 4, global step 250: 'train_loss' reached 0.99941 (best 0.99941), saving model to '/kaggle/working/lightning_logs/version_8/checkpoints/epoch=4-step=250.ckpt' as top 1\nINFO: Epoch 5, global step 300: 'train_loss' was not in top 1\nINFO: Epoch 6, global step 350: 'train_loss' was not in top 1\nINFO: Epoch 7, global step 400: 'train_loss' was not in top 1\nINFO: Epoch 8, global step 450: 'train_loss' was not in top 1\nINFO: Epoch 9, global step 500: 'train_loss' reached 0.97801 (best 0.97801), saving model to '/kaggle/working/lightning_logs/version_8/checkpoints/epoch=9-step=500.ckpt' as top 1\nINFO: Epoch 10, global step 550: 'train_loss' reached 0.90983 (best 0.90983), saving model to '/kaggle/working/lightning_logs/version_8/checkpoints/epoch=10-step=550.ckpt' as top 1\nINFO: Epoch 11, global step 600: 'train_loss' was not in top 1\nINFO: Epoch 12, global step 650: 'train_loss' was not in top 1\nINFO: Epoch 13, global step 700: 'train_loss' reached 0.89760 (best 0.89760), saving model to '/kaggle/working/lightning_logs/version_8/checkpoints/epoch=13-step=700.ckpt' as top 1\nINFO: Epoch 14, global step 750: 'train_loss' was not in top 1\nINFO: Epoch 15, global step 800: 'train_loss' was not in top 1\nINFO: Epoch 16, global step 850: 'train_loss' was not in top 1\nINFO: Epoch 17, global step 900: 'train_loss' was not in top 1\nINFO: Epoch 18, global step 950: 'train_loss' was not in top 1\nINFO: Epoch 19, global step 1000: 'train_loss' was not in top 1\nINFO: `Trainer.fit` stopped: `max_epochs=20` reached.\n[I 2024-05-02 09:16:00,963] Trial 7 finished with value: 71.11793629443801 and parameters: {'num_layers': 4, 'hidden_size': 31, 'lr': 0.0001, 'batch_size': 16}. Best is trial 1 with value: 10.073001049791909.\nINFO: GPU available: True (cuda), used: True\nINFO: TPU available: False, using: 0 TPU cores\nINFO: IPU available: False, using: 0 IPUs\nINFO: HPU available: False, using: 0 HPUs\n/opt/conda/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:74: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\nINFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\nINFO: \n  | Name  | Type        | Params | In sizes                                                      | Out sizes    \n----------------------------------------------------------------------------------------------------------------------\n0 | model | DeepARModel | 41.7 K | [[1, 1], [1, 1], [1, 156, 2], [1, 156], [1, 156], [1, 12, 2]] | [1, 1000, 12]\n----------------------------------------------------------------------------------------------------------------------\n41.7 K    Trainable params\n0         Non-trainable params\n41.7 K    Total params\n0.167     Total estimated model params size (MB)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ce6a8d2510b14b5a8454ef419cf897db"}},"metadata":{}},{"name":"stderr","text":"INFO: Epoch 0, global step 50: 'train_loss' reached 1.03778 (best 1.03778), saving model to '/kaggle/working/lightning_logs/version_9/checkpoints/epoch=0-step=50.ckpt' as top 1\nINFO: Epoch 1, global step 100: 'train_loss' reached 0.91861 (best 0.91861), saving model to '/kaggle/working/lightning_logs/version_9/checkpoints/epoch=1-step=100.ckpt' as top 1\nINFO: Epoch 2, global step 150: 'train_loss' reached 0.84682 (best 0.84682), saving model to '/kaggle/working/lightning_logs/version_9/checkpoints/epoch=2-step=150.ckpt' as top 1\nINFO: Epoch 3, global step 200: 'train_loss' reached 0.80668 (best 0.80668), saving model to '/kaggle/working/lightning_logs/version_9/checkpoints/epoch=3-step=200.ckpt' as top 1\nINFO: Epoch 4, global step 250: 'train_loss' reached 0.75463 (best 0.75463), saving model to '/kaggle/working/lightning_logs/version_9/checkpoints/epoch=4-step=250.ckpt' as top 1\nINFO: Epoch 5, global step 300: 'train_loss' reached 0.75270 (best 0.75270), saving model to '/kaggle/working/lightning_logs/version_9/checkpoints/epoch=5-step=300.ckpt' as top 1\nINFO: Epoch 6, global step 350: 'train_loss' reached 0.74453 (best 0.74453), saving model to '/kaggle/working/lightning_logs/version_9/checkpoints/epoch=6-step=350.ckpt' as top 1\nINFO: Epoch 7, global step 400: 'train_loss' was not in top 1\nINFO: Epoch 8, global step 450: 'train_loss' reached 0.74308 (best 0.74308), saving model to '/kaggle/working/lightning_logs/version_9/checkpoints/epoch=8-step=450.ckpt' as top 1\nINFO: Epoch 9, global step 500: 'train_loss' reached 0.71846 (best 0.71846), saving model to '/kaggle/working/lightning_logs/version_9/checkpoints/epoch=9-step=500.ckpt' as top 1\nINFO: Epoch 10, global step 550: 'train_loss' reached 0.70653 (best 0.70653), saving model to '/kaggle/working/lightning_logs/version_9/checkpoints/epoch=10-step=550.ckpt' as top 1\nINFO: Epoch 11, global step 600: 'train_loss' reached 0.69804 (best 0.69804), saving model to '/kaggle/working/lightning_logs/version_9/checkpoints/epoch=11-step=600.ckpt' as top 1\nINFO: Epoch 12, global step 650: 'train_loss' was not in top 1\nINFO: Epoch 13, global step 700: 'train_loss' reached 0.68698 (best 0.68698), saving model to '/kaggle/working/lightning_logs/version_9/checkpoints/epoch=13-step=700.ckpt' as top 1\nINFO: Epoch 14, global step 750: 'train_loss' reached 0.67487 (best 0.67487), saving model to '/kaggle/working/lightning_logs/version_9/checkpoints/epoch=14-step=750.ckpt' as top 1\nINFO: Epoch 15, global step 800: 'train_loss' reached 0.66698 (best 0.66698), saving model to '/kaggle/working/lightning_logs/version_9/checkpoints/epoch=15-step=800.ckpt' as top 1\nINFO: Epoch 16, global step 850: 'train_loss' reached 0.65374 (best 0.65374), saving model to '/kaggle/working/lightning_logs/version_9/checkpoints/epoch=16-step=850.ckpt' as top 1\nINFO: Epoch 17, global step 900: 'train_loss' was not in top 1\nINFO: Epoch 18, global step 950: 'train_loss' reached 0.63859 (best 0.63859), saving model to '/kaggle/working/lightning_logs/version_9/checkpoints/epoch=18-step=950.ckpt' as top 1\nINFO: Epoch 19, global step 1000: 'train_loss' reached 0.63258 (best 0.63258), saving model to '/kaggle/working/lightning_logs/version_9/checkpoints/epoch=19-step=1000.ckpt' as top 1\nINFO: `Trainer.fit` stopped: `max_epochs=20` reached.\n[I 2024-05-02 09:16:19,313] Trial 8 finished with value: 12.175440193237485 and parameters: {'num_layers': 3, 'hidden_size': 43, 'lr': 0.001, 'batch_size': 32}. Best is trial 1 with value: 10.073001049791909.\nINFO: GPU available: True (cuda), used: True\nINFO: TPU available: False, using: 0 TPU cores\nINFO: IPU available: False, using: 0 IPUs\nINFO: HPU available: False, using: 0 HPUs\n/opt/conda/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:74: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\nINFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\nINFO: \n  | Name  | Type        | Params | In sizes                                                      | Out sizes    \n----------------------------------------------------------------------------------------------------------------------\n0 | model | DeepARModel | 62.6 K | [[1, 1], [1, 1], [1, 156, 2], [1, 156], [1, 156], [1, 12, 2]] | [1, 1000, 12]\n----------------------------------------------------------------------------------------------------------------------\n62.6 K    Trainable params\n0         Non-trainable params\n62.6 K    Total params\n0.251     Total estimated model params size (MB)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4e2f7c70a0c94905848b9d1c3718487c"}},"metadata":{}},{"name":"stderr","text":"INFO: Epoch 0, global step 50: 'train_loss' reached 0.95515 (best 0.95515), saving model to '/kaggle/working/lightning_logs/version_10/checkpoints/epoch=0-step=50.ckpt' as top 1\nINFO: Epoch 1, global step 100: 'train_loss' reached 0.85411 (best 0.85411), saving model to '/kaggle/working/lightning_logs/version_10/checkpoints/epoch=1-step=100.ckpt' as top 1\nINFO: Epoch 2, global step 150: 'train_loss' reached 0.80133 (best 0.80133), saving model to '/kaggle/working/lightning_logs/version_10/checkpoints/epoch=2-step=150.ckpt' as top 1\nINFO: Epoch 3, global step 200: 'train_loss' reached 0.78159 (best 0.78159), saving model to '/kaggle/working/lightning_logs/version_10/checkpoints/epoch=3-step=200.ckpt' as top 1\nINFO: Epoch 4, global step 250: 'train_loss' reached 0.74517 (best 0.74517), saving model to '/kaggle/working/lightning_logs/version_10/checkpoints/epoch=4-step=250.ckpt' as top 1\nINFO: Epoch 5, global step 300: 'train_loss' reached 0.70355 (best 0.70355), saving model to '/kaggle/working/lightning_logs/version_10/checkpoints/epoch=5-step=300.ckpt' as top 1\nINFO: Epoch 6, global step 350: 'train_loss' reached 0.66107 (best 0.66107), saving model to '/kaggle/working/lightning_logs/version_10/checkpoints/epoch=6-step=350.ckpt' as top 1\nINFO: Epoch 7, global step 400: 'train_loss' reached 0.64534 (best 0.64534), saving model to '/kaggle/working/lightning_logs/version_10/checkpoints/epoch=7-step=400.ckpt' as top 1\nINFO: Epoch 8, global step 450: 'train_loss' reached 0.62297 (best 0.62297), saving model to '/kaggle/working/lightning_logs/version_10/checkpoints/epoch=8-step=450.ckpt' as top 1\nINFO: Epoch 9, global step 500: 'train_loss' reached 0.58114 (best 0.58114), saving model to '/kaggle/working/lightning_logs/version_10/checkpoints/epoch=9-step=500.ckpt' as top 1\nINFO: Epoch 10, global step 550: 'train_loss' was not in top 1\nINFO: Epoch 11, global step 600: 'train_loss' reached 0.57375 (best 0.57375), saving model to '/kaggle/working/lightning_logs/version_10/checkpoints/epoch=11-step=600.ckpt' as top 1\nINFO: Epoch 12, global step 650: 'train_loss' was not in top 1\nINFO: Epoch 13, global step 700: 'train_loss' reached 0.53447 (best 0.53447), saving model to '/kaggle/working/lightning_logs/version_10/checkpoints/epoch=13-step=700.ckpt' as top 1\nINFO: Epoch 14, global step 750: 'train_loss' was not in top 1\nINFO: Epoch 15, global step 800: 'train_loss' reached 0.52796 (best 0.52796), saving model to '/kaggle/working/lightning_logs/version_10/checkpoints/epoch=15-step=800.ckpt' as top 1\nINFO: Epoch 16, global step 850: 'train_loss' reached 0.48883 (best 0.48883), saving model to '/kaggle/working/lightning_logs/version_10/checkpoints/epoch=16-step=850.ckpt' as top 1\nINFO: Epoch 17, global step 900: 'train_loss' reached 0.47875 (best 0.47875), saving model to '/kaggle/working/lightning_logs/version_10/checkpoints/epoch=17-step=900.ckpt' as top 1\nINFO: Epoch 18, global step 950: 'train_loss' reached 0.45132 (best 0.45132), saving model to '/kaggle/working/lightning_logs/version_10/checkpoints/epoch=18-step=950.ckpt' as top 1\nINFO: Epoch 19, global step 1000: 'train_loss' was not in top 1\nINFO: `Trainer.fit` stopped: `max_epochs=20` reached.\n[I 2024-05-02 09:16:35,741] Trial 9 finished with value: 12.091072674874372 and parameters: {'num_layers': 5, 'hidden_size': 40, 'lr': 0.01, 'batch_size': 16}. Best is trial 1 with value: 10.073001049791909.\n","output_type":"stream"},{"name":"stdout","text":"Number of finished trials: 10\nBest trial:\n  Value: 10.073001049791909\n  Params: \n    num_layers: 3\n    hidden_size: 41\n    lr: 0.01\n    batch_size: 16\n182.81919741630554\n","output_type":"stream"}]}]}