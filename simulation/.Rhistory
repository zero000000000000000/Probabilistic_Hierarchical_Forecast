}
write.csv(simulated_set, file = 'D:/data_scenario1.csv')
diag(rep(2, 4))
rep(2,4)
df <- rbind(df, basef1, basef2)
# utility functions
library(forecast)
set.seed(42)
hts <- function(basis, sMat) {
t(sMat %*% t(basis))
}
cal_basef <- function(series, method = 'arima'){
series = hts(series, sMat)
apply(series, 2, function(series){
series = ts(series, frequency = 12)
if (method == 'arima'){
model = auto.arima(series)
}
if (method == 'ets'){
model = ets(series)
}
c(fitted(model), forecast(model, h=24)$mean)
})
}
# settings
sMat = rbind(matrix(c(1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1), 3, 4),
diag(rep(1, 4)))
# scenario
data1 = read.csv('D:/data_scenario1.csv', row.names = 1)
df = data.frame()
for (index in 1:100){
print(index)
series <- data1[data1$index==index,][1:300, 1:4]
basef1 <- data.frame(cal_basef(series, method='arima'),
method='arima',
index=index,
t=1:324)
basef2 <- data.frame(cal_basef(series, method='ets'),
method='ets',
index=index,
t=1:324)
df <- rbind(df, basef1, basef2)
}
# utility functions
library(forecast)
set.seed(42)
hts <- function(basis, sMat) {
t(sMat %*% t(basis))
}
cal_basef <- function(series, method = 'arima'){
series = hts(series, sMat)
apply(series, 2, function(series){
series = ts(series, frequency = 12)
if (method == 'arima'){
model = auto.arima(series)
}
if (method == 'ets'){
model = ets(series)
}
c(fitted(model), forecast(model, h=24)$mean)
})
}
# settings
sMat = rbind(matrix(c(1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1), 3, 4),
diag(rep(1, 4)))
# scenario
data1 = read.csv('D:/data_scenario1.csv', row.names = 1)
df = data.frame()
for (index in 1:5){
print(index)
series <- data1[data1$index==index,][1:300, 1:4]
basef1 <- data.frame(cal_basef(series, method='arima'),
method='arima',
index=index,
t=1:324)
basef2 <- data.frame(cal_basef(series, method='ets'),
method='ets',
index=index,
t=1:324)
df <- rbind(df, basef1, basef2)
}
write.csv(df, 'D:/basef1.csv')
# This code generates bottom level time series for the
# simulation study from a Gaussian stationary DGP
require(portes)
require(MASS)
require(tidyr)
require(tsibble)
set.seed(1989) #Set seed
m <- 4 # Number of bottom level
init<- 500 #Number of initial values to be removed
train<-500 # Size of training sample
H <- 3 #Maximum forecast horizon
L <- 4 #Pre-sample
R <- 1000 #Number of reps
N <- train+R+init+H+L-1 # Sample size
#Randomly generating errors from a Gaussian distribution
Bottom_pop_cov<-matrix(c(5,3.1,0.6,0.4,3.1,4,0.9,1.4,0.6,
0.9,2,1.8,0.4,1.4,1.8,3), nrow = m,
ncol = m)  #Covariance matrix
E <- mvrnorm(n = N+5, mu = rep(0, m), Sigma = Bottom_pop_cov) #Generate MVN disturbances
#Generating the bottom level series. Each series were generated from
#ARMA(p,d,q) model where the parameters were randomly selected from the
#defined parameter space
order_p <- sample(c(1,2), size = m, replace = TRUE)
order_d <- sample(c(0), size = m, replace = TRUE)
order_q <- sample(c(1,2), size = m, replace = TRUE)
Bottom_level <- matrix(0, nrow = N,  ncol = m)
AR_coef_store<-matrix(0,m,2)
MA_coef_store<-matrix(0,m,2)
statinv<-1 #flag for stationarity and invertibility
for (i in 1:m)
{
if (order_p[i]==0) {
AR_coef <- 0
} else {
while(!is.null(statinv)){
AR_coef <- runif(n=order_p[i], min = 0.3, max = 0.5)
statinv<-tryCatch(InvertQ(AR_coef)) #This will produce a warning if AR non-stationary
}
statinv<-1
}
AR_coef_store[i,1:length(AR_coef)]<-AR_coef #Keep AR coefficients
if (order_q[i]==0) {
MA_coef <- 0
} else {
while(!is.null(statinv)){
MA_coef <- runif(n=order_q[i], min = 0.3, max = 0.7)
statinv<-tryCatch(InvertQ(MA_coef)) #This will produce a warning if AR non-stationary
}
statinv<-1
}
MA_coef_store[i,1:length(MA_coef)]<-MA_coef #Keep AR coefficients
Bottom_level[,i] <- arima.sim(list(order=c(order_p[i],order_d[i],order_q[i]),
ar=AR_coef, ma=MA_coef), n = (N+5),
innov = E[,i])[2:(N+1)]
}
install.packages('portes')
install.packages('tsibble')
# This code generates bottom level time series for the
# simulation study from a Gaussian stationary DGP
install.packages('portes')
install.packages('tsibble')
require(portes)
require(MASS)
require(tidyr)
require(tsibble)
set.seed(1989) #Set seed
m <- 4 # Number of bottom level
init<- 500 #Number of initial values to be removed
train<-500 # Size of training sample
H <- 3 #Maximum forecast horizon
L <- 4 #Pre-sample
R <- 1000 #Number of reps
N <- train+R+init+H+L-1 # Sample size
#Randomly generating errors from a Gaussian distribution
Bottom_pop_cov<-matrix(c(5,3.1,0.6,0.4,3.1,4,0.9,1.4,0.6,
0.9,2,1.8,0.4,1.4,1.8,3), nrow = m,
ncol = m)  #Covariance matrix
E <- mvrnorm(n = N+5, mu = rep(0, m), Sigma = Bottom_pop_cov) #Generate MVN disturbances
#Generating the bottom level series. Each series were generated from
#ARMA(p,d,q) model where the parameters were randomly selected from the
#defined parameter space
order_p <- sample(c(1,2), size = m, replace = TRUE)
order_d <- sample(c(0), size = m, replace = TRUE)
order_q <- sample(c(1,2), size = m, replace = TRUE)
Bottom_level <- matrix(0, nrow = N,  ncol = m)
AR_coef_store<-matrix(0,m,2)
MA_coef_store<-matrix(0,m,2)
statinv<-1 #flag for stationarity and invertibility
for (i in 1:m)
{
if (order_p[i]==0) {
AR_coef <- 0
} else {
while(!is.null(statinv)){
AR_coef <- runif(n=order_p[i], min = 0.3, max = 0.5)
statinv<-tryCatch(InvertQ(AR_coef)) #This will produce a warning if AR non-stationary
}
statinv<-1
}
AR_coef_store[i,1:length(AR_coef)]<-AR_coef #Keep AR coefficients
if (order_q[i]==0) {
MA_coef <- 0
} else {
while(!is.null(statinv)){
MA_coef <- runif(n=order_q[i], min = 0.3, max = 0.7)
statinv<-tryCatch(InvertQ(MA_coef)) #This will produce a warning if AR non-stationary
}
statinv<-1
}
MA_coef_store[i,1:length(MA_coef)]<-MA_coef #Keep AR coefficients
Bottom_level[,i] <- arima.sim(list(order=c(order_p[i],order_d[i],order_q[i]),
ar=AR_coef, ma=MA_coef), n = (N+5),
innov = E[,i])[2:(N+1)]
}
# Eliminate a proportion of initial values
Bottom_level <- Bottom_level[-(1:init),]
Ut <- rnorm(n = N-init, mean = 0, sd = sqrt(28)) #u_t
Vt <- rnorm(n = N-init, mean = 0, sd = sqrt(22)) #v_t
##Inequality Check
VTot<-sum(Bottom_pop_cov)
VA<-sum(Bottom_pop_cov[1:2,1:2])+22
VB<-sum(Bottom_pop_cov[3:4,3:4])+22
VAA<-Bottom_pop_cov[1,1]+22/4+28
VAB<-Bottom_pop_cov[2,2]+22/4+28
VBA<-Bottom_pop_cov[3,3]+22/4+28
VBB<-Bottom_pop_cov[4,4]+22/4+28
if(VTot>VA){print('VTot larger than VA')}
if(VTot>VB){print('VTot larger than VB')}
if(VA>VAA){print('VA larger than VA')}
if(VA>VAB){print('VA larger than VAB')}
if(VB>VBA){print('VB larger than VBA')}
if(VB>VBB){print('VB larger than VBB')}
AA <- Bottom_level[,1]+Ut-0.5*Vt
AB <- Bottom_level[,2]-Ut-0.5*Vt
BA <- Bottom_level[,3]+Ut+0.5*Vt
BB <- Bottom_level[,4]-Ut+0.5*Vt
#Find aggregates
A=AA+AB
B=BA+BB
Tot=A+B
#Put into a wide format (for export to csv)
wide<-tibble(Time=1:(N-init),Tot,A,B,AA,AB,BA,BB)
write.csv(wide, "../Data/gaussian_stationary.csv",row.names = F)
write.csv(wide, "D:/gaussian_stationary.csv",row.names = F)
help arima.sim
help(arima.sim)
S<-matrix(c(1,1,1,1,
1,1,0,0,
0,0,1,1,
1,0,0,0,
0,1,0,0,
0,0,1,0,
0,0,0,1),7,4,byrow = T)
solve(t(S)%*%S,t(S))
solve(t(S)%*%S)
solve(t(S)%*%S)%*%t(S)
help("solve")
t(apply(S,1,sort))
S
apply(S,1,sort)
S
help('apply')
#Create and Evaluate reconciled forecasts
setwd('D:\\HierarchicalCode\\simulation')
#install.packages("rjson")
library(rjson)
# Load library
library(tidyverse)
library(mvtnorm)
library(Matrix)
#Clear workspace
rm(list=ls())
# CRPS with alpha = 1
crps<-function(y,x,xs){
dif1<-x-xs
dif2<-y-x
term1<-apply(dif1,1,function(v){sum(abs(v))})
term2<-apply(dif2,1,function(v){sum(abs(v))})
return(((-0.5*term1)+term2)/ncol(x))
}
#Energy score with alpha = 2
energy_score<-function(y,x,xs){
dif1<-x-xs
dif2<-y-x
term1<-apply(dif1,2,function(v){sum(v^2)})%>%sum
term2<-apply(dif2,2,function(v){sum(v^2)})%>%sum
return(((-0.5*term1)+term2)/ncol(x))
}
# W with shrinkage
shrink.estim <- function(res)
{
n<-nrow(res)
covm <- cov(res)
tar <- diag(diag(covm))
corm <- stats::cov2cor(covm)
xs <- scale(res, center = FALSE, scale = sqrt(diag(covm)))
xs <- xs[stats::complete.cases(xs),]
v <- (1/(n * (n - 1))) * (crossprod(xs^2) - 1/n * (crossprod(xs))^2)
diag(v) <- 0
corapn <- stats::cov2cor(tar)
d <- (corm - corapn)^2
lambda <- sum(v)/sum(d)
lambda <- max(min(lambda, 1), 0)
W <- lambda * tar + (1 - lambda) * covm
return(W)
}
# Produce summing matrix of new basis time series
transform.sMat <- function(sMat, basis_set){
m <- dim(sMat)[2]
if (length(basis_set) != m){
stop(simpleError(sprintf('length of basis set should be %d', m)))
}
S1 <- sMat[basis_set,]
S2 <- sMat[-basis_set,]
transitionMat <- solve(S1, diag(rep(1, m)))
rbind(S2 %*% transitionMat, diag(rep(1, m)))
}
# Return basis series index,that is vector v and u
forecast.basis_series <- function(sMat,
immu_set=NULL){
m <- dim(sMat)[2]
n <- dim(sMat)[1]
k <- length(immu_set)
# construct new basis time series
if (k > m) {
stop(simpleError(sprintf('length of basis set can not be bigger than %d', m)))
}
## select mutable series
immutable_basis <- sort(immu_set)
candidate_basis <- setdiff((n-m+1):n, immu_set)
if (all(immutable_basis >= n-m+1 )){
mutable_basis <- candidate_basis
} else {
mutable_basis <- c()
determined <- c()
i <- max(which(immutable_basis < n-m+1))
while (length(mutable_basis) != m-k) {
corresponding_leaves <- which(sMat[immutable_basis[i], ] != 0) + n - m
free_leaves <- setdiff(corresponding_leaves, c(immutable_basis, mutable_basis, determined))
if (length(free_leaves) == 0) stop(simpleError('the immu_set can not be used to describe the hierarchy'))
if (length(free_leaves) == 1) {
candidate_basis <- candidate_basis[candidate_basis != free_leaves[1]]
} else{
determined <- c(determined, free_leaves[1])
mutable_basis <- c(mutable_basis, free_leaves[2:length(free_leaves)])
candidate_basis <- candidate_basis[!(candidate_basis %in% free_leaves)]
}
i <- i - 1
if (i == 0) {
mutable_basis <- c(mutable_basis, candidate_basis)
}
}
}
return(list(mutable_basis=mutable_basis,immutable_basis=immutable_basis))
}
# Return the reconciled result
forecast.reconcile<-function(base_forecasts,
sMat,
weighting_matrix,
mutable_basis,
immutable_basis){
m <- dim(sMat)[2]
n <- dim(sMat)[1]
k <- length(immutable_basis)
new_basis <- c(sort(mutable_basis), immutable_basis)
sMat <- transform.sMat(sMat, new_basis)
print(sMat)
S1 <- sMat[1:(n-k),,drop=FALSE][,1:(m-k),drop=FALSE]
S2 <- sMat[1:(n-m),,drop=FALSE][,(m-k+1):m,drop=FALSE]
determined <- setdiff(1:n, new_basis)
mutable_series <- c(determined, mutable_basis)
mutable_weight <- solve(weighting_matrix[mutable_series,,drop=FALSE][,mutable_series,drop=FALSE])
mutable_base <- cbind(base_forecasts[,determined,drop=FALSE] - t(S2 %*% t(base_forecasts[,immutable_basis,drop=FALSE])),
base_forecasts[,sort(mutable_basis),drop=FALSE])
reconciled_mutable <- solve(t(S1) %*% mutable_weight %*% S1) %*% t(S1) %*% mutable_weight %*% t(mutable_base)
reconciled_y <- t(sMat %*% rbind(reconciled_mutable, t(base_forecasts[,immutable_basis,drop=FALSE])))
new_index <- c(determined, new_basis)
print(new_index)
return(reconciled_y[,order(new_index)])
}
############# Set constant
evalN<-10 #Number of evaluation periods
Q<-1000 #Number of draws to estimate energy score
N<-500 #Training sample size
M<-7 #Number of series
#Set up S matrix
S<-matrix(c(1,1,1,1,
1,1,0,0,
0,0,1,1,
1,0,0,0,
0,1,0,0,
0,0,1,0,
0,0,0,1),7,4,byrow = T)
#Predefine some reconciliation matrices
SG_bu<-S%*%cbind(matrix(0,4,3),diag(rep(1,4)))
SG_ols<-S%*%solve(t(S)%*%S,t(S))
# Set the type of the input dataset
z<-2
dataset_type<-read.csv('D:\\HierarchicalCode\\simulation\\Config\\Config_dataset_type.csv')
generate<-dataset_type$generate[z]
rootbasef<-dataset_type$rootbasef[z]
depj<-dataset_type$basefdep[z]
#Read in data
data<-read.csv(paste('D:\\HierarchicalCode\\simulation\\Data\\Simulated_Data_',generate,'.csv',sep=''))
# Read the Base forecast results
fc<-fromJSON(file = paste("D:\\HierarchicalCode\\simulation\\Base_Forecasts\\",generate,"_",rootbasef,".json",sep=''))
for (i in 1:10){
names(fc[[i]])<-c('fc_mean','fc_var','resid','fitted')
}
# Calculate sd
for (i in 1:10){
sd_list<-NULL
for (j in 1:7){
sd_list<-c(sd_list,sqrt(fc[[i]]$fc_var[j]))
}
fc[[i]]$fc_sd<-sd_list
}
for (i in 1:10){
k<-NULL
for (j in 1:7){
m<-matrix(fc[[i]]['resid'][[1]][[j]],nrow=1,ncol=500,byrow=T)
k<-rbind(k,m)
}
fc[[i]]$resid_mat<-k
fc[[i]]$fc_Sigma_sam<-cov(t(k))
fc[[i]]$fc_Sigma_shr<-shrink.estim(t(k))
}
# ES Without immutable series
Base<-rep(NA,evalN)
BottomUp<-rep(NA,evalN)
OLS<-rep(NA,evalN)
WLS<-rep(NA,evalN)
JPP<-rep(NA,evalN)
MinTShr<-rep(NA,evalN)
MinTSam<-rep(NA,evalN)
# ES With immutable series
OLSv<-rep(NA,evalN)
WLSv<-rep(NA,evalN)
MinTShrv<-rep(NA,evalN)
MinTSamv<-rep(NA,evalN)
# CRPS without immutable series
Basec<-rep(NA,M*evalN)
BottomUpc<-rep(NA,M*evalN)
OLSc<-rep(NA,M*evalN)
WLSc<-rep(NA,M*evalN)
JPPc<-rep(NA,M*evalN)
MinTShrc<-rep(NA,M*evalN)
MinTSamc<-rep(NA,M*evalN)
# CRPS With immutable series
OLScv<-rep(NA,M*evalN)
WLScv<-rep(NA,M*evalN)
MinTShrcv<-rep(NA,M*evalN)
MinTSamcv<-rep(NA,M*evalN)
# Start creating and evaluating
# Set seed
set.seed(12)
for (i in 1:1){
#Get realisation
y1<-data[N+i,]
y2<-as.matrix(y1)
y3<-matrix(rep(y2,Q),nrow=7,byrow=F)
y<-y3
#Base forecasts
fc_i<-fc[[i]]
if (depj=='Independent'){
#Gaussian independent
fc_mean<-fc_i$fc_mean
fc_sd<-fc_i$fc_sd
x<-matrix(rnorm((Q*M),mean=fc_mean,sd=fc_sd),M,Q)
xs<-matrix(rnorm((Q*M),mean=fc_mean,sd=fc_sd),M,Q)
}else if(depj=='Joint'){
#Gaussian dependent
fc_mean<-fc_i$fc_mean
fc_sigma<-fc_i$fc_Sigma_sam
x<-t(rmvnorm(Q,fc_mean,fc_sigma))
xs<-t(rmvnorm(Q,fc_mean,fc_sigma))
}
# Set the immutable point and Get the basis series
basis_lis<-forecast.basis_series(S,immu_set=c(1))
'''
#Base forecast
Base[i]<-energy_score(y,x,xs)
Basec[((i-1)*M+1):(i*M)]<-crps(y,x,xs)
#Bottom up
newx<-SG_bu%*%x
newxs<-SG_bu%*%xs
BottomUp[i]<-energy_score(y,newx,newxs)
BottomUpc[((i-1)*M+1):(i*M)]<-crps(y,newx,newxs)
'''
for (i in 1:1){
#Get realisation
y1<-data[N+i,]
y2<-as.matrix(y1)
y3<-matrix(rep(y2,Q),nrow=7,byrow=F)
y<-y3
#Base forecasts
fc_i<-fc[[i]]
if (depj=='Independent'){
#Gaussian independent
fc_mean<-fc_i$fc_mean
fc_sd<-fc_i$fc_sd
x<-matrix(rnorm((Q*M),mean=fc_mean,sd=fc_sd),M,Q)
xs<-matrix(rnorm((Q*M),mean=fc_mean,sd=fc_sd),M,Q)
}else if(depj=='Joint'){
#Gaussian dependent
fc_mean<-fc_i$fc_mean
fc_sigma<-fc_i$fc_Sigma_sam
x<-t(rmvnorm(Q,fc_mean,fc_sigma))
xs<-t(rmvnorm(Q,fc_mean,fc_sigma))
}
# Set the immutable point and Get the basis series
basis_lis<-forecast.basis_series(S,immu_set=c(1))
#OLS
newx<-SG_ols%*%x
newxs<-SG_ols%*%xs
OLS[i]<-energy_score(y,newx,newxs)
OLSc[((i-1)*M+1):(i*M)]<-crps(y,newx,newxs)
newx <- t(forecast.reconcile(t(x),
S,
diag(rep(1,M)),
basis_lis$mutable_basis,
basis_lis$immutable_basis))
newxs <- t(forecast.reconcile(t(xs),
S,
diag(rep(1,M)),
basis_lis$mutable_basis,
basis_lis$immutable_basis))
OLSv[i]<-energy_score(y,newx,newxs)
OLScv[((i-1)*M+1):(i*M)]<-crps(y,newx,newxs)
}
